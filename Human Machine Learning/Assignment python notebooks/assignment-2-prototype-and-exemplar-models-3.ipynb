{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc7131f-429a-4b29-877c-39ffcbebc1de",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Assignment 2: Prototype and exemplar models\n",
    "\n",
    "**Please do not consult external resources other than the readings for this assignment. Prototype and exemplar models have been extended beyond the implementations of the models presented in this homework. We want you to think about the implications of the representations posited by these models and what they mean for providing explanations for human behavior.**\n",
    "\n",
    "Make sure you have done the required readings for this homework (which were also required readings for class):\n",
    "\n",
    "* Murphy, G. L. (2002). The big book of concepts. Cambridge, MA: MIT\n",
    "  Press [chapter 2 & 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18ecd3-8e73-4689-a781-157f8080f189",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data representation\n",
    "\n",
    "Both the models in this assignment represent observations from the\n",
    "world as collection of attributes, each of which has a corresponding\n",
    "value. For example, a ball can have a radius and a weight attribute\n",
    "with values 12 cm (4.72 inches) and 500 g (1.1 lbs) respectively.\n",
    "\n",
    "We can have many examples of these items to learn from. We can\n",
    "represent these items as rows of a matrix, and attributes as columns\n",
    "of a matrix. For example, if we had 3 balls with the following values:\n",
    "\n",
    "| Ball # | Radius (cm) | Weight (g) |\n",
    "|--------|-------------|------------|\n",
    "|      0 |           5 |        200 |\n",
    "|      1 |           3 |        400 |\n",
    "|      2 |          20 |       1000 |\n",
    "\n",
    "\n",
    "We can represent this information in a matrix, that looks like this:\n",
    "\n",
    "The Ball # is implicitly encoded in the matrix as the index of the\n",
    "row. Each column contains the values of each attribute. The names of\n",
    "each attributes can be retrieved using the indices of the columns.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "5 & 200 \\\\\n",
    "3 & 400 \\\\\n",
    "20 & 1000 \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f53e1-42cf-4a36-a9e5-927937264942",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Models\n",
    "\n",
    "In this assignment, we will be replicating the modeling results from Medin and Schaffer (1978).\n",
    "\n",
    "[Medin, D. L., & Schaffer, M. M. (1978). Context theory of\n",
    "classification learning. Psychological Review, 85(3), 207–238.\n",
    "https://doi.org/10.1037/0033-295X.85.3.207](https://groups.psych.northwestern.edu/medin/documents/MedinSchaffer1978PsychRev.pdf)\n",
    "\n",
    "Here are the stimuli in the paper:\n",
    "\n",
    "![](static/medin-and-schaffer-1978-stimuli.png)\n",
    "\n",
    "Let's tabulate the stimuli from the paper into a matrix. The wonders of small data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdba90a6-4481-4e88-96cd-4e306b7ee58d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 1, 0],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 1],\n",
       "       [0, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "raw_stimuli = [1, 1, 1, 0, 0,\n",
    "               1, 0, 1, 0, 0,\n",
    "               1, 0, 1, 1, 0,\n",
    "               1, 1, 0, 1, 0,\n",
    "               0, 1, 1, 1, 0,\n",
    "               1, 1, 0, 0, 1,\n",
    "               0, 1, 1, 0, 1,\n",
    "               0, 0, 0, 1, 1,\n",
    "               0, 0, 0, 0, 1]\n",
    "\n",
    "raw_stimuli = np.array(raw_stimuli).reshape(9, -1)\n",
    "raw_stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c90dc9-8b2f-4c72-bca5-5d549649947f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Note that the rows represent the various examples in\n",
    "the training set. The first four columns are the features from the\n",
    "paper, and the last column represents the category.\n",
    "\n",
    "We are representing category $A$ using $0$ and category $B$ using $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81775b2e-65a6-4906-b1e8-ac4a614f6b6d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prototype model\n",
    "\n",
    "The prototype model posits that humans generate a prototype of the\n",
    "various categories they encounter in the world, and use this prototype\n",
    "to determine the category of an observation in the world.\n",
    "\n",
    "These models are supervised, i.e. they need labelled data to function.\n",
    "Our data is split into two categories, $A$ and $B$.\n",
    "\n",
    "In this assignment, we will look at a simple prototype model that\n",
    "assumes that people keep track of the mean values of attributes of a\n",
    "category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bfe7ac-6cc8-4d28-bad2-2c69b358a53d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Implement the `get_prototype()` function**\n",
    "\n",
    "This function should take the dataset, and the category for which we want to build a \n",
    "prototype and return the prototypes for each designated category (mean value of attributes for each designated category). \n",
    "\n",
    "Specifically, the return value for the items in the dataset for this assigment\n",
    "should be a numpy vector of length 4.\n",
    "\n",
    "Make sure the category column is not included in the prototype.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "108e8be8-b5d5-4cdc-92d4-845eb1b51073",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_prototype(dataset, category):\n",
    "    # dataset: numpy matrix\n",
    "    # category: number    \n",
    "    # pass\n",
    "    # print(dataset)\n",
    "    data_without_category = dataset[dataset[:, -1] == category]\n",
    "    # print(data_without_category)\n",
    "    mean = np.mean(data_without_category[:, :-1], axis=0)\n",
    "    # print(data_without_category[:, :-1])\n",
    "    # print(mean.shape)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97bd5e3-33b1-438a-bba0-67c1428ddb05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's display the prototypes of the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e7c8c4e-e192-4cc2-8fda-1093767c6013",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: 0, Prototype: [0.8 0.6 0.8 0.6]\n",
      "Category: 1, Prototype: [0.25 0.5  0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "ALL_CATEGORIES = np.arange(0, np.max(raw_stimuli[:, 4]) + 1)\n",
    "for category in ALL_CATEGORIES:\n",
    "    prototype = get_prototype(raw_stimuli, category)\n",
    "    print(f\"Category: {category}, Prototype: {prototype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6eb69-917f-4aeb-a8fd-a656d740383c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To infer a category from an example in the wild, we will need a\n",
    "function calculates the distance from a set of stimuli to a prototype.\n",
    "This prototype model will use the Euclidean distance measure, which is\n",
    "\n",
    "$$\n",
    "d_{iP} = \\sqrt{\\sum_{k = 1}^{N} (x_{ik} - P_k)^2}\n",
    "$$\n",
    "\n",
    "where $i$ is the index of the item, $P$ represents a specific\n",
    "prototype, $k$ represents an attribute, and $N$ is the total number of\n",
    "attributes.\n",
    "\n",
    "**Implement the `distance_from_prototype()` function**\n",
    "\n",
    "The return value should be a vector of distances, one for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2dd7b30-ebae-4e52-a9dc-15653711b8cb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def distance_from_prototype(observations, prototype):\n",
    "    # prototype: the prototype of a category, excluding the category column.\n",
    "    # observations: numpy matrix, excluding the category column.   \n",
    "    # pass  \n",
    "    distances = np.sqrt(np.sum((observations - prototype) ** 2, axis=1))\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7c3ad-8ad1-453a-bb48-a0e2b5c5b2be",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's see how far away the items in the dataset are from the prototype\n",
    "of category $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1744b6b-d6a9-4d4e-89c2-5b564c030307",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77459667, 0.89442719, 0.77459667, 1.        , 1.        ,\n",
       "       1.09544512, 1.09544512, 1.34164079, 1.41421356])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_prototype = get_prototype(raw_stimuli, 0)\n",
    "raw_stimuli_without_category = raw_stimuli[:,:4]\n",
    "distance_from_prototype(raw_stimuli_without_category, a_prototype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f380d-91de-4be7-aea2-55415b600428",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we have all the information we need to calculate category\n",
    "probabilities for a set of stimuli. To get probabilities from\n",
    "distances from prototypes we use the following formula.\n",
    "\n",
    "$$\n",
    "P(A|x_i) = 1 - \\frac{d_{iP_A}}{\\sum_{c \\in \\text{Values(C)}}d_{iP_c}}\n",
    "$$\n",
    "\n",
    "where $C$ is the set of all categories.\n",
    "\n",
    "Note that this is predicting the probability of category $A$, given\n",
    "that we have stimulus $x_i$.\n",
    "\n",
    "**Implement the `prototype_predict_category_probabilities(dataset, observations)` function**\n",
    "\n",
    "You will have to, for each category:\n",
    "\n",
    "* determine the prototypes using the `get_prototype()`\n",
    "* determine the distances of the observations from the prototypes using the `distance_from_prototype()`\n",
    "\n",
    "Using these functions, you should calculate $P(c|x_i)$ for each stimuli\n",
    "in each category.\n",
    "\n",
    "The return value should be a matrix of probabities of, where the rows\n",
    "represent the observations and the columns represent the categories.\n",
    "\n",
    "Do not worry about efficiency, just accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "852e2169-3e9c-43d5-b858-50442d79db5f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prototype_predict_category_probabilities(dataset, observations):\n",
    "    # dataset: numpy matrix\n",
    "    # observations: numpy matrix with observations to predict the categories of\n",
    "    # The global variable ALL_CATEGORIES is an array with all the category values\n",
    "    # FILL IN\n",
    "    # pass\n",
    "    prototype_probabilities = np.zeros((observations.shape[0], len(ALL_CATEGORIES)))\n",
    "    \n",
    "    for category in ALL_CATEGORIES:\n",
    "        prototype = get_prototype(dataset, category)\n",
    "        distances = distance_from_prototype(observations, prototype)\n",
    "        prototype_probabilities[:, category] = distances\n",
    "\n",
    "    sum_di_Pc = np.sum(prototype_probabilities, axis=1, keepdims=True)\n",
    "    prototype_probabilities = 1 - (prototype_probabilities / sum_di_Pc)\n",
    "    \n",
    "    return prototype_probabilities\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c05e4bb-965b-43f9-882e-57ba3a0ecc58",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's predict the categories of the training set itself.\n",
    "\n",
    "**Test case**\n",
    "\n",
    "The first four rows of the matrix should be\n",
    "\n",
    "```python\n",
    "[[0.6075119 , 0.3924881 ],\n",
    " [0.57273642, 0.42726358],\n",
    " [0.64247257, 0.35752743],\n",
    " [0.54523913, 0.45476087],\n",
    " # ...\n",
    " ]\n",
    "```\n",
    "\n",
    "This means that for the first observation, the $P(A)$ is 0.61, and the\n",
    "$P(B)$ is 0.39. For the second observation, the $P(A)$ is 0.57, and the\n",
    "$P(B)$ is 0.43, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c353fdba-869a-4639-a37b-9f3a86f78cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6075119  0.3924881 ]\n",
      " [0.57273642 0.42726358]\n",
      " [0.64247257 0.35752743]\n",
      " [0.54523913 0.45476087]\n",
      " [0.54523913 0.45476087]\n",
      " [0.46918161 0.53081839]\n",
      " [0.46918161 0.53081839]\n",
      " [0.41917462 0.58082538]\n",
      " [0.31866518 0.68133482]]\n",
      "Test results ==>  True True True True\n"
     ]
    }
   ],
   "source": [
    "result = prototype_predict_category_probabilities(raw_stimuli, raw_stimuli_without_category)\n",
    "print(result)\n",
    "print(\"Test results ==> \", result[0][0].round(2) == 0.61, result[0][1].round(2) == 0.39, result[1][0].round(2) == 0.57, result[1][1].round(2) == 0.43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f164d-fa14-4338-9bf1-0054b82bc62d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Exemplar models\n",
    "\n",
    "Exemplar models posit no abstraction process. They theorize that humans store\n",
    "examples of concepts (along with their categories) and use similarity to those\n",
    "examples to categorize a particular observation.\n",
    "\n",
    "It's important to note that exemplar models have a fitting process,\n",
    "where constants are fit using human data. Each attribute $k$ will have\n",
    "a parameter $s_k$, where $0 \\leq s_k \\leq 1$. $s_k$ represents a\n",
    "penalty for an attribute not matching. We will represent this in the\n",
    "code as a `penalties` parameter array/vector, which we will later fit.\n",
    "\n",
    "To determine the similarity between two items, the exemplar model uses this formula.\n",
    "\n",
    "$$\n",
    "  \\text{sim}(x, y) = \\prod_{k = 1}^{N} \\begin{cases}\n",
    "    1 & \\text{if } x_k = y_k \\\\\n",
    "    s_k & \\text{if } x_k \\neq y_k\n",
    "  \\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d968c93-d1ba-4fb7-9c23-478ac3df2a56",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Implement `exemplar_similarity()`**\n",
    "\n",
    "The return value should be a number representing the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6cb2f1e2-8505-497b-9644-1737325cc27b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def exemplar_similarity(item_a, item_b, penalties):\n",
    "    # item_a: vector representing an observation\n",
    "    # item_b: vector representing an observation\n",
    "    # penalties: vector of s_k mentioned above\n",
    "    # pass\n",
    "    exemplar_similarity = 1.0\n",
    "    loop_range = len(item_a)\n",
    "    \n",
    "    for item in range(loop_range):\n",
    "        if item_a[item] != item_b[item]:\n",
    "            exemplar_similarity *= penalties[item]\n",
    "    \n",
    "    return exemplar_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c441d6-3e28-490d-b18c-e52a96fbba0c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Test case**\n",
    "\n",
    "We can test the code using two items from the dataset, by using hard\n",
    "coded penalty values of 0.5 for each attribute. Since these examples\n",
    "differ by one attribute, the similarity should be 0.5 (the penalty value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bbbfb997-3f76-4beb-904b-8da9408d7d4b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim([1 1 1 0], [1 0 1 0]) = 0.5\n"
     ]
    }
   ],
   "source": [
    "sim = exemplar_similarity(raw_stimuli_without_category[0], raw_stimuli_without_category[1], [0.5, 0.5, 0.5, 0.5])\n",
    "print(f\"sim({raw_stimuli_without_category[0]}, {raw_stimuli_without_category[1]}) = {sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9f12a-0c32-4749-aa20-8ba814864fc9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we can use the similarity information to calculate the\n",
    "probability that any observation is in category $A$:\n",
    "\n",
    "$$\n",
    "P(A | x_i) = \\frac{\\sum_{j = 1}^{M_A} \\text{sim}(x_i, A_j)}\n",
    "{\\sum_{c \\in \\text{Values}(C)}\\sum_{j = 1}^{M_c} \\text{sim}(x_i, c_j)\n",
    "}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $C$ is the set of all categories\n",
    "* $M_c$ is the number of examples in category $c$\n",
    "* $c_j$ is the $j$-th example in category $c$\n",
    "\n",
    "**Implement `exemplar_predict_category_probabilities()`**\n",
    "\n",
    "Again, do not worry about efficiency, just accuracy. The return value\n",
    "should be similar to `prototype_predict_category_probabilities()`, a\n",
    "matrix of probabities with rows representing observations, and\n",
    "columns representing the categories. Feel free to define and use\n",
    "helper functions if you need them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "56634fdf-f9fd-47a3-8c22-5570ab381244",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def exemplar_predict_category_probabilities(dataset, observations, penalties):\n",
    "    # dataset: numpy matrix\n",
    "    # penalties: the penalites for the similarity function\n",
    "    \n",
    "    # The global variable ALL_CATEGORIES is an array with all the category values\n",
    "    # pass\n",
    "    \n",
    "    exemplar_probabilities = np.zeros((observations.shape[0], len(ALL_CATEGORIES)))\n",
    "    \n",
    "    for i, observation in enumerate(observations):\n",
    "        similarities_per_category = np.zeros(len(ALL_CATEGORIES))\n",
    "\n",
    "        for data in dataset:\n",
    "            similarity = exemplar_similarity(observation, data[:4], penalties)\n",
    "            similarities_per_category[int(data[4])] += similarity\n",
    "                # print(data, data[:4], data[4])\n",
    "            \n",
    "        exemplar_probabilities[i, :] = similarities_per_category / np.sum(similarities_per_category)\n",
    "    \n",
    "    return exemplar_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46f79e-346b-4ead-8772-6c2eae09151c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's predict the categories of the dataset using itself. We will hard\n",
    "code the penalties for now.\n",
    "\n",
    "**Test case**\n",
    "\n",
    "The first four rows of the matrix should be\n",
    "\n",
    "```python\n",
    "[[0.65454545, 0.34545455],\n",
    " [0.72      , 0.28      ],\n",
    " [0.7826087 , 0.2173913 ],\n",
    " [0.65217391, 0.34782609],\n",
    " # ...\n",
    " ]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "30553d08-3990-4c98-85fe-52bc3d04b789",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65454545 0.34545455]\n",
      " [0.72       0.28      ]\n",
      " [0.7826087  0.2173913 ]\n",
      " [0.65217391 0.34782609]\n",
      " [0.65217391 0.34782609]\n",
      " [0.48       0.52      ]\n",
      " [0.48       0.52      ]\n",
      " [0.34883721 0.65116279]\n",
      " [0.27272727 0.72727273]]\n",
      "Test results ==>  True True True True\n",
      "Test results ==>  True True True True\n"
     ]
    }
   ],
   "source": [
    "result = exemplar_predict_category_probabilities(raw_stimuli, raw_stimuli_without_category, [0.5, 0.5, 0.5, 0.5])\n",
    "print(result)\n",
    "print(\"Test results ==> \", result[0][0].round(2) == 0.65, result[0][1].round(2) == 0.35, result[1][0].round(2) == 0.72, result[1][1].round(2) == 0.28)\n",
    "print(\"Test results ==> \", result[2][0].round(2) == 0.78, result[2][1].round(2) == 0.22, result[3][0].round(2) == 0.65, result[3][1].round(2) == 0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188041e3-9cf8-46ec-968a-628fcb7d068b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parameter fitting\n",
    "\n",
    "We've been using hard-coded penalties (0.5 each) so far. The exemplar\n",
    "model is specified such that these parameters should be fit by human\n",
    "data. Let's tabulate the human data from the paper and fit the\n",
    "parameters.\n",
    "\n",
    "We will use `scipy`'s `curve_fit()` function to fit our parameters. It\n",
    "uses the\n",
    "[Levenberg–Marquardt](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm)\n",
    "algorithm to find good parameter values. It returns a tuple with the\n",
    "parameters and errors, which we discard here.\n",
    "\n",
    "Install `scipy` using `pip3 install scipy` if it is not installed.\n",
    "\n",
    "You will not need to write any code for this section, just use the\n",
    "fitted penalties to answer the questions at the end of this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "058cfeb0-9ec8-468c-9fa3-6a8ba0d1721a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalties: [0.16065222 0.37070295 0.18720096 0.07054756]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.79800606, 0.20199394],\n",
       "       [0.90000202, 0.09999798],\n",
       "       [0.96740734, 0.03259266],\n",
       "       [0.89120799, 0.10879201],\n",
       "       [0.88263076, 0.11736924],\n",
       "       [0.23400825, 0.76599175],\n",
       "       [0.21197944, 0.78802056],\n",
       "       [0.13042492, 0.86957508],\n",
       "       [0.04188833, 0.95811167]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO? why todo here even though we are not required to write anything in this function? hmm.\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Human A probabilities for Medin and Schaffer (1978)\n",
    "\n",
    "human_a_probs = np.array([0.78, 0.88, 0.81, 0.88, 0.81, 0.16, 0.16, 0.12, 0.03])\n",
    "\n",
    "def exemplar_predict_a_prob(dataset, *penalties):\n",
    "    dataset_without_categories = dataset[:, :4]\n",
    "    predictions = exemplar_predict_category_probabilities(dataset, dataset_without_categories, penalties)\n",
    "    a_predictions = predictions[:, 0]\n",
    "    return a_predictions\n",
    "\n",
    "penalties, _ = curve_fit(exemplar_predict_a_prob, raw_stimuli, human_a_probs, p0=[0.5, 0.5, 0.5, 0.5])\n",
    "print(f\"Penalties: {penalties}\")\n",
    "\n",
    "exemplar_predict_category_probabilities(raw_stimuli, raw_stimuli_without_category, penalties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4db6f-ad56-4d4e-aa41-47e8b2d976bc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "Respond to each question in one or two paragraphs. Remember, we are\n",
    "looking for connections to psychological plausibility in humans.\n",
    "  \n",
    "* Create a table summarizing the predictions of the prototype model\n",
    "  and the exemplar model for the 9 training stimuli (the 5 instances\n",
    "  of category $A$ and the 4 instances of category $B$, and also the\n",
    "  performance of humans during the test phase. Be sure you are\n",
    "  consistent in either providing $P(A)$ for all 9 instances, or\n",
    "  providing $P(A)$ for the first 5 and $P(B)$ for the\n",
    "  last 4. The human data can be found in the lecture slides.\n",
    "\n",
    "* How well do each of the models predict the human data? Describe\n",
    "  patterns in the data. Then, answer this question quantitatively. For\n",
    "  example, you might compute the correlation between each model's\n",
    "  predictions and the human data, and report these two values. Or you\n",
    "  might compute the root mean square deviation (RMSD) between each\n",
    "  model's predictions and the human data. or you might use another\n",
    "  measure of model fit. Justify the measure you choose.\n",
    "\n",
    "* As you learned in lecture, the key difference between the models is\n",
    "  with regard to $A$ stimuli $\\begin{bmatrix} 1 & 1 & 1 & 0\n",
    "  \\end{bmatrix}$ and $\\begin{bmatrix} 1 & 0 & 1 & 0\n",
    "  \\end{bmatrix}$. Refer to the lecture slides about the\n",
    "  differential predictions the models make on these stimuli. Was that\n",
    "  the case in your results? Why does this pattern indicate that the\n",
    "  exemplar model may better capture how people represent categories?\n",
    "\n",
    "* When we have multiple models of human cognition, we typically want\n",
    "  to determine which models are better than others. The prototype\n",
    "  model presented performs poorly compared to the exemplar model. On\n",
    "  the other hand, the prototype model is parameter-free and the\n",
    "  exemplar models contains 4 parameters (i.e., the s(i)s) that need to\n",
    "  be fit to human data. When evaluating these two models to determine\n",
    "  which model is a better and more useful model of human cognition,\n",
    "  how should we penalize the exemplar model for the free parameters it\n",
    "  requires, and the flexibility that they provide? You can provide\n",
    "  quantitative and/or verbal descriptions for your answer.\n",
    "\n",
    "* What kind of human categorization behavior might be beyond either\n",
    "  the prototype model or the exemplar model? You need only consider\n",
    "  the limits of one of the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f84cd2",
   "metadata": {},
   "source": [
    "# Answers\n",
    "\n",
    "1. Below is the table that collates the predictions of prototype model, exemplar model, and the human testing phase.    \n",
    "\n",
    "| Stimuli | Prototype P(A) | Prototype P(B) | Exemplar P(A) | Exemplar P(B)  |  Human-Observed | Human-Predicted |\n",
    "|---------|----------------|----------------|---------------|----------------|-----------------|-----------------|\n",
    "|     4A  |       0.61     |       0.39     |       0.80    |       0.20     |       0.78      |       0.79      |\n",
    "|     7A  |       0.57     |       0.43     |       0.90    |       0.10     |       0.88      |       0.94      |\n",
    "|    15A  |       0.64     |       0.36     |       0.97    |       0.03     |       0.81      |       0.97      |\n",
    "|    13A  |       0.55     |       0.45     |       0.89    |       0.11     |       0.88      |       0.86      |\n",
    "|     5A  |       0.55     |       0.45     |       0.88    |       0.12     |       0.81      |       0.86      |\n",
    "|    12B  |       0.47     |       0.53     |       0.23    |       0.77     |       0.84      |       0.76      |\n",
    "|     2B  |       0.47     |       0.53     |       0.21    |       0.79     |       0.84      |       0.76      |\n",
    "|    14B  |       0.42     |       0.58     |       0.13    |       0.87     |       0.88      |       0.93      |\n",
    "|    10B  |       0.32     |       0.68     |       0.04    |       0.96     |       0.97      |       0.97      |\n",
    "\n",
    "\n",
    "2. As we can clearly see, the prototype model is quiet far in terms of the predictions compared to human data vs exemplar model is. We can see that the exemplar model much more clearly predicts how a human predicts the result based on the stimuli provided. For example, for the stimuli 4A, Prototype model predicted a 0.61 probability whereas, the exemplar model predicts a 0.80 which is closer to the human testing result of 0.79. For some stimuli, the predicts are even an exact match barring some drastic errors in calculations. For a better quantitative view, let's calculate the root mean square deviation value for both the models. \n",
    "   1. For the prototype model - \n",
    "      1. Total deviations i.e. how far each value is from the human observed values = |0.61 - 0.78|, |0.57 - 0.88|, |0.64 - 0.81|, |0.55 - 0.88|, |0.55 - 0.81|, |0.53 - 0.84|, |0.53 - 0.84|, |0.58 - 0.88|, |0.68 - 0.97| = |0.61 - 0.78| = 0.17, 0.31, 0.17, 0.33, 0.26, 0.31, 0.31, 0.30, 0.29\n",
    "      1. Squared Deviations - 0.0289, 0.0961, 0.0289, 0.1089, 0.0676, 0.0961, 0.0961, 0.09, 0.0841\n",
    "      2. square root of the Mean of these squared deviations = Sum(0.0289, 0.0961, 0.0289, 0.1089, 0.0676, 0.0961, 0.0961, 0.09, 0.0841) / 9 is approximately 25.8%\n",
    "   2. For the exemplar model - \n",
    "      1. Total deviations i.e. how far each value is from the human observed values = |0.80 - 0.78|, |0.90 - 0.88|, |0.97 - 0.81|, |0.89 - 0.88|, |0.88 - 0.81|, |0.77 - 0.84|, |0.79 - 0.84|, |0.87 - 0.88|, |0.96 - 0.97| = 0.02, 0.02, 0.16, 0.01, 0.07, 0.07, 0.05, 0.01, 0.01\n",
    "      2. Squared Deviations - 0.0004, 0.0004, 0.0256, 0.0001, 0.0049, 0.0049, 0.0025, 0.0001, 0.0001\n",
    "      3. Mean of these squared deviations = Sum(0.0004, 0.0004, 0.0256, 0.0001, 0.0049, 0.0049, 0.0025, 0.0001, 0.0001) / 9 = 0.00433\n",
    "      4. Root mean square deviation = square root of 0.00433 which is approximately 6.6%.\n",
    "   3. What the RMSD does is it measures the average deviation of predictions from actual human data for both the models. For the prototype mode, we see the value is 28% which is much higher than the value for exemplar model which is 6.6%. The higher the value, the more the deviation. So, we can say quantitatively, that the exemplar model has a better fit to human data as compared to prototype model.\n",
    "3. The key difference between the models can be seen from 4A and 7A as mentioned in the question - [ 1 1 1 0] and [1 0 1 0]. The prototype model tries to categorize it based on the how close a stimulus comes to a prototype and in that, if we see the prediction ratings for these two examples, the values are almost 50-50, equally split - 61% and 57% probabilities, as if the model is not able to categorize it to the correct one - A. Whereas, the exemplar model has done an excellent job of categorizing it to A with a 80% to 90% probability. Because exemplar model works on the idea of similarity between two items. Even though A is [1 1 1 1] and we know that 4A is a closer item to A than 7A is, but even then, we can see a 80% probability for 4A and 90% for 7A that they are in the category A. This was the case in my results too. Surprisingly, that is also closer to how a human would categorize them based on the test results as provided in the lecture as well as the paper by Medin and Schafer. This pattern suggests that the exemplar model captures how people categorize stimuli better than prototype model because it reflects the fact that human categorization is often instance-based rather than a singular, generalized prototype.\n",
    "4. Both the models Prototype and exemplar have their pros and cons. The prototype model with it's simple flow without parameter runs into cons like less accuracy and less likely to overfit but then again it is not complex enough to capture the way a human might think and process stimuli. On the other hand, with exemplar model, even though it has the 4 parameters that account for its accuracy, it also makes it complex enough to capture the nuanced variations in human cognition. That is also evident from the RMSD exercise we did in the previous answers. To quantitatively generate a report of which model might be better we can use scores like precision, accuracy, f1-score, BIC or AIC values, recall scores that are generally used to evaluate any Machine Learning model. Especially values like Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) might come in handy because they penalize the models for the number of parameters they end up using. For example, Lower BIC values might mean a better balance between model fit and model complexity. Even though, we know for a fact that exemplar model is closer to the human values, the number of parameter used should also mean we get a better model fit based on the BIC values.\n",
    "5. Even though both prototype and exemplar models are good at categorizing binary as well as multi dimensional data as was the case in our experiment, all the data used was linear. The main limitations of the models would arise when dealing with non-linear data. One might think that these models also suck at categorizing outliers but exemplar model is pretty good at categorizing them even though prototype might not be. Humans are more than capable of handling non-linear, complex, dynamic, context-sensitive types of data as well as can deal well with outlier but prototype model might be at a disadvantage. To fix these errors or to tackle them atleast, in the lecture we talked about other models like Rulex model which is a hybrid model to better handle complex data.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "name": "assignment-2-prototype-and-exemplar-models.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
