{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6606e3",
   "metadata": {
    "collapsed": false,
    "id": "fc6606e3",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Assignment 1: Decision Trees\n",
    "\n",
    "**Please do not consult external resources for this assignment.**\n",
    "\n",
    "Decision trees have been well-explored in the field of machine\n",
    "learning. However, we expect you to provide your opinions and ideas on\n",
    "how decision trees can be used to model human learning.\n",
    "\n",
    "The required reading for this assignment is **Witten, I. H., et al. (2017). Data mining: Practical machine learning tools and techniques. Amsterdam: Elsevier. (pp. 105-113, 75-78)**. \n",
    "\n",
    "Optional readings:\n",
    "\n",
    "* More technical: *Duda, R. O., et al. (2001). Pattern recognition (2nd ed.). New York: Wiley. (chap. 6)*\n",
    "* Familiar if youâ€™ve taken AI: *Russell, R., & Norvig, P. (2021) Artificial intelligence: A modern approach (4th ed.). Prentice-Hall. (chap 19, pp. 656-665)*\n",
    "* Classic treatment: *Mitchell, T. M. (1997). Machine learning. McGraw-Hill. (chap. 3, pp. 52-60)*\n",
    "\n",
    "These readings are available on Canvas under Files.\n",
    "\n",
    "In this assignment, we are going to use the [ID3\n",
    "Algorithm](https://en.wikipedia.org/wiki/ID3_algorithm) to induce a\n",
    "decision tree from the classic dataset in [Quinlan,\n",
    "1986](https://link.springer.com/content/pdf/10.1007%2FBF00116251.pdf).\n",
    "\n",
    "Please download quinlan-1986.csv (found in the files directory) and\n",
    "store it in your local directory.\n",
    "\n",
    "![](static/quinlan-1986-dataset.png)\n",
    "\n",
    "**Some of the code will be filled in for you, and in other places\n",
    "pseudo code will be provided. As a part of this assignment, please\n",
    "fill in the pseudocode. At the end of this notebook, you'll have a\n",
    "generated decision tree. In addition, answer the questions listed at\n",
    "the end of the assignment**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a58da0-6a70-4d08-9ca1-5b6c437ea30c",
   "metadata": {
    "collapsed": false,
    "id": "a5a58da0-6a70-4d08-9ca1-5b6c437ea30c",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Programming notes\n",
    "\n",
    "This assignment will require the use of Python, and the numpy and\n",
    "pandas libraries. Please refer to the following tutorials if you are\n",
    "not familiar with them.\n",
    "\n",
    "* [Python, if you know another programming language](https://learnxinyminutes.com/docs/python/)\n",
    "* [Python, if you are new to programming](https://www.codecademy.com/learn/learn-python-3)\n",
    "* [Numpy](https://scipy-lectures.org/intro/numpy/index.html)\n",
    "* [Pandas](https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e575b",
   "metadata": {
    "collapsed": false,
    "id": "541e575b",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Dependencies and library.\n",
    "\n",
    "Please ensure that you have `pandas` and `numpy` installed. If not,\n",
    "please run the following lines to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37ed244b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37ed244b",
    "outputId": "3153f372-9a70-47d7-cefb-e58c72190c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/pennywise97/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in /Users/pennywise97/anaconda3/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/pennywise97/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pennywise97/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/pennywise97/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pennywise97/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy\n",
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c4e78-9fbb-47f4-b674-9f7e6dddf383",
   "metadata": {
    "collapsed": false,
    "id": "836c4e78-9fbb-47f4-b674-9f7e6dddf383",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49a104c2",
   "metadata": {
    "id": "49a104c2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e794fd",
   "metadata": {
    "collapsed": false,
    "id": "35e794fd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Loading & Preprocessing\n",
    "\n",
    "Next, we load the dataset and separate the class (outcome) data from\n",
    "the rest of the dataset for implementation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "725a2d6b",
   "metadata": {
    "id": "725a2d6b"
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"files/quinlan-1986.csv\")\n",
    "class_ = d[\"class\"]\n",
    "training_data = d.drop(\"class\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bfd36",
   "metadata": {
    "collapsed": false,
    "id": "dc3bfd36",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## ID3 Algorithm Functions\n",
    "\n",
    "ID3 needs to know if splitting a decision tree perfectly partitions\n",
    "the input. For that reason, we define a function to check whether a\n",
    "series of class values has only one unique value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fffdaa29-f2c2-43bd-9042-36fb79c605b0",
   "metadata": {
    "id": "fffdaa29-f2c2-43bd-9042-36fb79c605b0"
   },
   "outputs": [],
   "source": [
    "def is_pure(class_series):\n",
    "    return len(np.unique(class_series)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e8f4f-0ed1-4ae1-862a-21c9af35c835",
   "metadata": {
    "collapsed": false,
    "id": "528e8f4f-0ed1-4ae1-862a-21c9af35c835",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "ID3 utilizes the metric of information gain to determine the best\n",
    "features to utilize for split the data at each node of the decision\n",
    "tree. Information gain is calculated using entropy, in which entropy\n",
    "is a measure of impurity and nonhomogeneity in the dataset. For\n",
    "example, entropy is 0 if all datapoints in a dataset belong to a\n",
    "single class.\n",
    "\n",
    "**Please complete the `entropy()` and `information_gain()` functions\n",
    "using the following equations.** Do not worry about computational\n",
    "efficiency, just make sure you get the logic right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd07fa-8f1d-4cb1-985b-e610f86e1227",
   "metadata": {
    "collapsed": false,
    "id": "05fd07fa-8f1d-4cb1-985b-e610f86e1227",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Entropy\n",
    "\n",
    "The following equation describes how to calculate entropy for a dataset with\n",
    "$c$ classes.\n",
    "\n",
    "$$E(S) = \\sum_{i=1}^{c} -p_{i}\\log_{2}p_{i}$$\n",
    "\n",
    "Utilize the above equation to implement the `entropy()` function. The\n",
    "function takes in as input `sample_counts`, which is an array\n",
    "containing the total number of samples associated with each class.\n",
    "\n",
    "For example, if there are 4 Ns and 5 Ps, then `sample_count` would be `[4, 5]`.\n",
    "\n",
    "The function should return a number. Note that log(0) is not\n",
    "mathematically defined, use the value 0 in this case.\n",
    "\n",
    "**Test cases**\n",
    "\n",
    "* `entropy(np.array([1, 0, 0, 0]))` == 0\n",
    "* `entropy(np.array([1, 1, 0, 0]))` == 1\n",
    "* `entropy(np.array([1, 1, 0, 10]))` ~ 0.8167\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "852e348c",
   "metadata": {
    "id": "852e348c"
   },
   "outputs": [],
   "source": [
    "def entropy(sample_counts):\n",
    "    total_samples = np.sum(sample_counts)\n",
    "    probabilities = sample_counts / total_samples\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    entropy = -np.sum([pi * np.log2(pi) for pi in probabilities])\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94023f2a",
   "metadata": {
    "collapsed": false,
    "id": "94023f2a",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Information Gain\n",
    "\n",
    "`information_gain()` takes in as input a DataFrame with `cases` (*S*), a\n",
    "vector with `classes` for the cases, and the `column_name` (*A*) that\n",
    "we would like to determine the *gain* of splitting on:\n",
    "\n",
    "Calculate the information gain using the following equation:\n",
    "\n",
    "$$\\text{Gain}(S,A) = \\text{Entropy}(S) - \\sum_{v\\in \\text{Values}(A)} \\frac{|S_{v}|}{|S|} \\text{Entropy}(S_{v})$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\text{Entropy}(S)$ calculates the entropy across the entire sample S,\n",
    "* $\\text{Values}(A)$ is the set of all possible values for an\n",
    "  attribute $A$, and\n",
    "* $S_{v}$ is the subset of $S$ for which attribute $A$ has value $v$.\n",
    "\n",
    "You may find these functions useful:\n",
    "\n",
    "* [`Series.value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html)\n",
    "* [`pd.crosstab()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html)\n",
    "\n",
    "The function should return the gain value (number) for splitting `cases` on\n",
    "`column_name`, using the `entropy()` function you defined earlier.\n",
    "\n",
    "**Test cases**\n",
    "\n",
    "The first four splits for these columns should have these values (from the slides)\n",
    "\n",
    "* outlook == `0.24674981977443933`\n",
    "* temperature == `0.02922256565895487`\n",
    "* humidity == `0.15183550136234159`\n",
    "* windy == `0.04812703040826949`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "815cd7d2",
   "metadata": {
    "id": "815cd7d2"
   },
   "outputs": [],
   "source": [
    "def information_gain(cases, classes, column_name):\n",
    "    \n",
    "    total_entropy = entropy(classes.value_counts().values)\n",
    "    values, counts = np.unique(cases[column_name], return_counts=True)\n",
    "    weighted_sum = 0\n",
    "    \n",
    "    for value, count in zip(values, counts):\n",
    "        subset = cases[cases[column_name] == value]\n",
    "        subset_classes = classes[subset.index]\n",
    "        subset_entropy = entropy(subset_classes.value_counts().values)\n",
    "        weighted_sum += (count / len(cases)) * subset_entropy\n",
    "    \n",
    "    information_gain = total_entropy - weighted_sum\n",
    "\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7658687",
   "metadata": {
    "collapsed": false,
    "id": "e7658687",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## ID3 Training\n",
    "\n",
    "Finally, we require a training loop that keeps splitting on variables (features) in the dataset, sorted by information gain, until we have a perfect tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "563431b6",
   "metadata": {
    "id": "563431b6"
   },
   "outputs": [],
   "source": [
    "def train_id3(node, X, y):\n",
    "    # if the dataset is perfectly separated, then stop, add a leaf node\n",
    "    if is_pure(y):\n",
    "        node_name = y.iloc[0] # the outcome value\n",
    "        node[\"children\"].append(node_name)\n",
    "    else:\n",
    "\n",
    "        # calculate information gain for splitting on each variable.\n",
    "        igs = []\n",
    "        for col in X.columns:\n",
    "            igs.append(information_gain(X, y, col))\n",
    "        # get the feature with the max information gain\n",
    "        max_index = np.argmax(igs)\n",
    "        feature_name = X.columns[max_index]\n",
    "\n",
    "        # split the dataset that feature. Now add a child node for each\n",
    "        # variable of that feature, and recursively complete the\n",
    "        # process.\n",
    "        grouped = X.groupby(feature_name)\n",
    "        children_nodes = []\n",
    "        for key, subset_X in grouped:\n",
    "            child_node = dict(name=f\"{feature_name}={key}\", children=[])\n",
    "            node[\"children\"].append(child_node)\n",
    "            subset_y = y[X[feature_name] == key]\n",
    "            train_id3(child_node, subset_X, subset_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7cb7a",
   "metadata": {
    "collapsed": false,
    "id": "1cb7cb7a",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Generating the decision tree\n",
    "\n",
    "The code is structured so that a Node is a dictionary with a name and\n",
    "a list of children. Let's induce the decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "330fe640",
   "metadata": {
    "id": "330fe640"
   },
   "outputs": [],
   "source": [
    "node = dict(name=\"root\", children=[])\n",
    "train_id3(node, training_data, class_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be2f30",
   "metadata": {
    "collapsed": false,
    "id": "c4be2f30",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Visualizing the decision tree\n",
    "\n",
    "The node dictionary is populated with each node's children provided as\n",
    "a list.\n",
    "\n",
    "We can now visualize the decision tree using two ways. The first way\n",
    "is to pretty print the dictionary. Another way is to use the `graphviz`\n",
    "package and generate the DOT source for it to graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff116a8c",
   "metadata": {
    "collapsed": false,
    "id": "ff116a8c",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Visualize using pretty print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "066ee88f",
   "metadata": {
    "id": "066ee88f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'root',\n",
      " 'children': [{'name': 'outlook=overcast', 'children': ['P']},\n",
      "              {'name': 'outlook=rain',\n",
      "               'children': [{'name': 'windy=False', 'children': ['P']},\n",
      "                            {'name': 'windy=True', 'children': ['N']}]},\n",
      "              {'name': 'outlook=sunny',\n",
      "               'children': [{'name': 'humidity=high', 'children': ['N']},\n",
      "                            {'name': 'humidity=normal', 'children': ['P']}]}]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pp\n",
    "pp(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79f754",
   "metadata": {
    "collapsed": false,
    "id": "ad79f754",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Visualize using graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b86fe9d",
   "metadata": {
    "id": "8b86fe9d"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"527pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 527.00 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-256 523,-256 523,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"236.5,-252 182.5,-252 182.5,-216 236.5,-216 236.5,-252\"/>\n",
       "<text text-anchor=\"middle\" x=\"209.5\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- 29761143 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>29761143</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"111,-180 0,-180 0,-144 111,-144 111,-180\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.5\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">outlook=overcast</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;29761143 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;29761143</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M182.32,-220.65C160.25,-210.61 128.61,-196.23 102.45,-184.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"103.64,-181.04 93.09,-180.09 100.75,-187.41 103.64,-181.04\"/>\n",
       "</g>\n",
       "<!-- 75556785 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>75556785</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"253,-180 166,-180 166,-144 253,-144 253,-180\"/>\n",
       "<text text-anchor=\"middle\" x=\"209.5\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">outlook=rain</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;75556785 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>0&#45;&gt;75556785</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M209.5,-215.7C209.5,-207.98 209.5,-198.71 209.5,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"213,-190.1 209.5,-180.1 206,-190.1 213,-190.1\"/>\n",
       "</g>\n",
       "<!-- 57880508 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>57880508</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"387.5,-180 289.5,-180 289.5,-144 387.5,-144 387.5,-180\"/>\n",
       "<text text-anchor=\"middle\" x=\"338.5\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">outlook=sunny</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;57880508 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>0&#45;&gt;57880508</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M236.54,-218.33C254.35,-208.66 278,-195.83 298.03,-184.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"299.83,-187.97 306.95,-180.12 296.49,-181.82 299.83,-187.97\"/>\n",
       "</g>\n",
       "<!-- 95743282 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>95743282</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"63.5,-108 9.5,-108 9.5,-72 63.5,-72 63.5,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"36.5\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">P</text>\n",
       "</g>\n",
       "<!-- 29761143&#45;&gt;95743282 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>29761143&#45;&gt;95743282</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.8,-143.7C48.69,-135.9 46.14,-126.51 43.78,-117.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47.14,-116.84 41.14,-108.1 40.38,-118.67 47.14,-116.84\"/>\n",
       "</g>\n",
       "<!-- 36115465 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>36115465</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"169,-108 82,-108 82,-72 169,-72 169,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.5\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">windy=False</text>\n",
       "</g>\n",
       "<!-- 75556785&#45;&gt;36115465 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>75556785&#45;&gt;36115465</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M188.74,-143.7C178.26,-134.97 165.39,-124.24 154,-114.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"155.95,-111.82 146.03,-108.1 151.47,-117.19 155.95,-111.82\"/>\n",
       "</g>\n",
       "<!-- 11418819 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>11418819</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"271.5,-108 187.5,-108 187.5,-72 271.5,-72 271.5,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"229.5\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">windy=True</text>\n",
       "</g>\n",
       "<!-- 75556785&#45;&gt;11418819 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>75556785&#45;&gt;11418819</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M214.44,-143.7C216.67,-135.9 219.35,-126.51 221.83,-117.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"225.23,-118.68 224.61,-108.1 218.5,-116.76 225.23,-118.68\"/>\n",
       "</g>\n",
       "<!-- 64275118 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>64275118</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"152.5,-36 98.5,-36 98.5,0 152.5,0 152.5,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">P</text>\n",
       "</g>\n",
       "<!-- 36115465&#45;&gt;64275118 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>36115465&#45;&gt;64275118</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M125.5,-71.7C125.5,-63.98 125.5,-54.71 125.5,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"129,-46.1 125.5,-36.1 122,-46.1 129,-46.1\"/>\n",
       "</g>\n",
       "<!-- 53824076 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>53824076</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"256.5,-36 202.5,-36 202.5,0 256.5,0 256.5,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"229.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">N</text>\n",
       "</g>\n",
       "<!-- 11418819&#45;&gt;53824076 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>11418819&#45;&gt;53824076</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M229.5,-71.7C229.5,-63.98 229.5,-54.71 229.5,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"233,-46.1 229.5,-36.1 226,-46.1 233,-46.1\"/>\n",
       "</g>\n",
       "<!-- 60796538 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>60796538</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"387.5,-108 289.5,-108 289.5,-72 387.5,-72 387.5,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"338.5\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">humidity=high</text>\n",
       "</g>\n",
       "<!-- 57880508&#45;&gt;60796538 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>57880508&#45;&gt;60796538</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M338.5,-143.7C338.5,-135.98 338.5,-126.71 338.5,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"342,-118.1 338.5,-108.1 335,-118.1 342,-118.1\"/>\n",
       "</g>\n",
       "<!-- 75654507 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>75654507</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"519,-108 406,-108 406,-72 519,-72 519,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.5\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">humidity=normal</text>\n",
       "</g>\n",
       "<!-- 57880508&#45;&gt;75654507 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>57880508&#45;&gt;75654507</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M368.83,-143.88C385.2,-134.64 405.58,-123.13 423.15,-113.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"425.06,-116.15 432.05,-108.19 421.62,-110.06 425.06,-116.15\"/>\n",
       "</g>\n",
       "<!-- 58261524 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>58261524</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"365.5,-36 311.5,-36 311.5,0 365.5,0 365.5,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"338.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">N</text>\n",
       "</g>\n",
       "<!-- 60796538&#45;&gt;58261524 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>60796538&#45;&gt;58261524</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M338.5,-71.7C338.5,-63.98 338.5,-54.71 338.5,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"342,-46.1 338.5,-36.1 335,-46.1 342,-46.1\"/>\n",
       "</g>\n",
       "<!-- 2262094 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>2262094</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"489.5,-36 435.5,-36 435.5,0 489.5,0 489.5,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">P</text>\n",
       "</g>\n",
       "<!-- 75654507&#45;&gt;2262094 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>75654507&#45;&gt;2262094</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M462.5,-71.7C462.5,-63.98 462.5,-54.71 462.5,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"466,-46.1 462.5,-36.1 459,-46.1 466,-46.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x121b0c3d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random # for generating node ids\n",
    "import graphviz\n",
    "\n",
    "def make_dot_internal(node, lines, i):\n",
    "    if isinstance(node, str):\n",
    "        lines.append(f'{i} [label=\"{node}\"];')\n",
    "    if isinstance(node, dict):\n",
    "        lines.append(f'{i} [label =\"{node[\"name\"]}\"];')\n",
    "        for j, child in enumerate(node[\"children\"]):\n",
    "            child_num = random.randint(0, 100000000)\n",
    "            make_dot_internal(child, lines, child_num)\n",
    "            lines.append(f'{i} -> {child_num};')\n",
    "\n",
    "def make_dot(node):\n",
    "    lines = []\n",
    "    make_dot_internal(node, lines, 0)\n",
    "    node_data = \"\\n\".join(lines)\n",
    "    return f\"digraph Tree {{ node[shape=box];\\n{node_data} }}\"\n",
    "\n",
    "graphviz.Source(make_dot(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729734a7",
   "metadata": {
    "collapsed": false,
    "id": "729734a7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "Respond to each question in one or two short paragraphs. Remember, we\n",
    "are looking for connections to psychological plausibility in humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1825e16",
   "metadata": {
    "collapsed": false,
    "id": "c1825e16",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1. What does your decision tree look like? Please include the code\n",
    "  you used to induce the tree, and a visualization (text or image) of\n",
    "  the tree.    \n",
    "    - My Decision tree generated from the \"quinlan-1986.csv\" dataset would typically be visualized as a series of nodes, where each node represents a decision based on a feature, leading to branches that represent different outcomes. The decision tree image is right above this cell and the code used to visualize it is also present, reused the same code as provided with the notebook.\n",
    "\n",
    "1. Comment on the ID3 algorithm for inducing decision trees. Is the\n",
    "  algorithm psychologically plausible? Which parts of the algorithm do\n",
    "  you think are psychologically plausible, and which parts are not?\n",
    "  What limitations do you see in decision trees? What kind of\n",
    "  concepts can or can't they learn? Do you see the same limitations in\n",
    "  humans?\n",
    "    - The ID3 algorithm seems psychologically plausible in a few aspects such as step by step reasoning and the overall decision making process. We as humans also tend to select most informative features/attributes, that provide us the most information gain just like ID3 selects attributes. We also take things step by step where every decision might be based on the outcome of a previous decision, which is similar to how ID3 generates the decision tree, incrementally. \n",
    "    \n",
    "    - The parts which i think are not psychologically plausible are the information gain calculations. Humans do not strictly make decisions based on some probability calculation that suggests this is the best decision and sometimes the decisions we make are influenced by the ones made in the past. We might also resort to heuristics and shortcuts while making our decisions and not necessarily precise calculations. Decision trees work better with hierarchical data that can be split into sequence of binary splits.\n",
    "    \n",
    "    - The one limitation that stands out is \"over-fitting\" where the model just copies the training data and is incapable of making correct decisions on a test dataset. Decision trees can over-fit on the data, this can often be the case when the tree becomes too deep. Where the noise that is introduced with a large dataset can be captured ignoring the underlying pattern. This is not how we typically generalize or make decisions, humans can often ignore noise or irrelevant details. Decision tree might also struggle with complex problems, humans on the other hand can handle such complex situations and make great judgement calls resulting in accurate decision making process.\n",
    "    \n",
    "    - Decision trees can learn concepts that are hierarchical, linearly seperable concepts/ideas/data. They struggle to learn non-linearly seperable concepts, they might make mistakes on continuous data without pre-processing i.e. with noise. As compared to humans, we can learn on noisy data or non-linearly seperable data as well as complex problems. Decision trees might also perform worse when we talk about making decisions that require global knowledge whereas human can perform better. Then again, there can be many factors at play in the human decision making process which might not be the case for ID3. \n",
    "\n",
    "1. What advantages do you see in decision trees? Do you see the same\n",
    "  behavior in humans?  \n",
    "      - The advantages i see in decision trees is definitely explainability, interpretability, cost effective because they tend to make greedy decisions. Decision trees are transparent while making decisions, humans can be biased. Decision trees can explain the decision process easily to humans and we can interpret a decision tree without much efforts too. Humans on the other hand might not always make decisions based on a precise probability calculation and it can be difficult to trace back and figure out how one ended at a decision. just like humans though, decision trees make decisions step by step, in a hierarchical fashion based on the most importatnt factors.\n",
    "\n",
    "1. One advantage of a decision tree is possibly higher *explainability*, i.e., it is easy for a human to understand why the model makes the classifications it makes. Comment on how the depth of the tree might affect the explainability.  \n",
    "    - A shallow tree with less number of nodes and levels might be easier to understand than a dense, multi-level tree with a lot of information and a lot of nodes. A shallow tree is easier to understand because of the small number of decisions. This reflects how humans prefer simple explanations with fewer steps to complex explanations.\n",
    "\n",
    "2. Note that the dataset above was perfectly separable. How would you\n",
    "  expand on the ID3 algorithm, or decision trees in general, to handle\n",
    "  noise in the data? Would this extension make decision trees a more\n",
    "  feasible model of human cognition? Why or why not?  \n",
    "      - As discussed in the class, pruning can be used to handle noise in a decision tree. Pruning is a data compression technique used in Decision tree to remove the sections of the tree that are non-critical or redundant. This would make sense and bring it closer to human cognition in a way how humans learn to ignore outliers or one-off cases while forming a decision. It also alligns more with human cognition how we tend to generalise rather than memorise.\n",
    "\n",
    "1. Note that the dataset above only contained categorical values. How\n",
    "  would you expand the ID3 algorithm, or decision trees in general, to\n",
    "  handle continuous values in the data? Would this extension make\n",
    "  decision trees a more feasible model of human cognition? Why or why\n",
    "  not?  \n",
    "     - ID3 for decision tree works very well with categorical values as we have seen, to expand it to handle continuous data, one would introduce range or splits in the continuous data. Humans have ranges for temperate or number like low, medium, high and that helps one deal better with continuous data and the same can be introduced for decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc9955",
   "metadata": {
    "collapsed": false,
    "id": "19bc9955",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Credits\n",
    "\n",
    "Equations are referenced from Chapter 3: Decision Tree Learning from\n",
    "Mitchell, T. M. (1997).\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "Please submit this Jupyter notebook including your code and your\n",
    "answers to the questions to Assignment 1 on Canvas.\n",
    "\n",
    "The code should not depend on any system specific configuration."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "name": "Assignment_1-_Decision_Trees.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
