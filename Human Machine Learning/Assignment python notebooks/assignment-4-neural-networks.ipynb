{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ee49ce-df3f-45aa-9850-578c444deb25",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Assignment 4: Neural Networks\n",
    "## Part 1: Manual calculation of neural network output and gradient descent \n",
    "\n",
    "The figure below shows a 2-layer, feed-forward neural network with two\n",
    "hidden-layer nodes and one output node. $x_1$ and $x_2$ are the two inputs,\n",
    "and $o_1$ is the output. For the following questions, assume the learning\n",
    "rate is $0.5$.  Each node also has a bias input value ($x_0$) of $1$.\n",
    "Assume there is a sigmoid ($\\sigma(x)$) activation function at the hidden\n",
    "layer nodes and at the output layer node.\n",
    "\n",
    "There are a variety of activation functions, each with different pros and cons.\n",
    "We are using the sigmoid activation function for this assignment. It has the\n",
    "property of clamping the output between 0 and 1. Note that sigmoid function is\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "and its derivative is\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "**Note that this is not the update rule. You will find this derivative in the\n",
    "update rule equation in the slides.**\n",
    "\n",
    "![](static/neural_network_manual_calculation.png)\n",
    "\n",
    "Calculate the output values at nodes $h_1$, $h_2$ and $o_1$ of this\n",
    "network for input $x_1 = 1, x_2 = 0$. Remember that bias term $x_0 = 1$.\n",
    "Each unit produces as its output the real value computed by the unit's\n",
    "associated sigmoid function.\n",
    "\n",
    "Then calculate one step of backpropagation, for the same input and target\n",
    "output $t = 1$ for $o_1$. Compute updated weights for the 3 connections\n",
    "into output layer, and then for the 6 connections into the hidden layer. There\n",
    "should be 9 updated weights in total. Make sure you understand when the\n",
    "activation function for a layer is applied in a neural network, and use the\n",
    "derivative of the activation function combined with the learning rate to\n",
    "determine the change in weights.\n",
    "\n",
    "People use many different functions as the loss function, depending on the task\n",
    "and the properties of the output data. For this question, use the L1 loss\n",
    "function to determine the error for backpropagation. L1 is defined as\n",
    "\n",
    "$$\n",
    "  \\text{L1} = \\frac{1}{n} \\sum^n_{i = 1} t_i - o_i\n",
    "$$\n",
    "\n",
    "where n is the number of dimensions in the output layer. It's a simple function\n",
    "calculating the difference between the target vector and the output vector. In\n",
    "this case, the output node is a single real value, so $n = 1$. And the loss\n",
    "becomes\n",
    "\n",
    "$$\n",
    "  \\text{L1} =  t - o_1\n",
    "$$\n",
    "\n",
    "Feel free to refer to the slides, which contain all the equations you need to\n",
    "complete part 1. **Note that the equations in the slides use\n",
    "the sigmoid activation function and L1 loss, referring to them might make this process easier**. You can also seek and read other\n",
    "sources sure you fully understand how neural networks work. Your TA is also\n",
    "glad to answer any questions.\n",
    "\n",
    "Submit your final answers, along with the steps to calculate them. If you\n",
    "consult external resources for the assignment, please include a link or\n",
    "citation to them too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab4e8d0",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "a. Weight calculation\n",
    "\n",
    "    The weights are - wh2x0 = 2, wh2x1 = -1.5, wh2x2 = 3, wh1x0 = 1.5, wh1x1 = -2.5, wh1x2 = 1, wo1x0 = -1, wo1h1 = 1, wo1h2 = 0.5 and the values of bias i.e. x0 = 1, and x1 = 1 and x2 = 0.\n",
    "    The output value at h1 = summation of w_ix_i where i = 0 to N.\n",
    "    that is summation of these values = wh1x0 * x0 = 1.5 * 1 = 1.5, wh1x1 * x1 = -2.5 * 1 = -2.5, wh1x2 * x2 = 1 * 0 = 0\n",
    "    = 1.5 + (-2.5) + 0\n",
    "    = -1\n",
    "    Now, applying the activation function to the value (-1), activation function is a sigmoid = 1 / 1 + e ^ -x\n",
    "    i.e. = 1 / 1 + e ^ -(-1) = 1 / 1 + e = 1 / 1 + 2.7 ~ 0.27\n",
    "\n",
    "    The output value at h2 = summation of w_ix_i where i = 0 to N.\n",
    "    that is summation of these values = wh2x0 * x0 = 2 * 1 = 2, wh2x1 * x1 = -1.5 * 1 = -1.5, wh2x2 * x2 = 3 * 0 = 0\n",
    "    = 2 + (-1.5) + 0\n",
    "    = 0.5\n",
    "    Now, applying the activation function to the value (0.5), activation function is a sigmoid = 1 / 1 + e ^ -x\n",
    "    i.e. = 1 / 1 + e ^ -(0.5) =  1 / 1 + 0.61 ~ 0.62\n",
    "\n",
    "    The output value at 01 = summation of w_ix_i where i = 0 to N.\n",
    "    that is summation of these values = wo1x0 * x0 = -1 * 1 = -1, wo1h1 * h1 = 1 * 0.27 = 0.27, wo1h2 * h2 = 0.5 * 0.65 = 0.32\n",
    "    = -1 + 0.27 + 0.32\n",
    "    = -0.41\n",
    "    Now, applying the activation function to the value (0.5), activation function is a sigmoid = 1 / 1 + e ^ -x\n",
    "    i.e. = 1 / 1 + e ^ -(-0.41) ~ 0.39\n",
    "\n",
    "    Hence, the approximate output values at h1, h2, and o1 are 0.27, 0.62, 0.39 (reduced to the second decimal place).\n",
    "\n",
    "    The loss calculation for target output t = 1 is defined as 1/n summation of differences between target and output. So, in our case, the Loss is:\n",
    "    = L1 = t − o1 = 1−0.39 ~ 0.61\n",
    "\n",
    "b. Back propagation:\n",
    "    The error calculation Delta at output node = derivation of activation function * loss\n",
    "    Delta = sigma(x) * (1 - sigma(x)) * (t1 - o1) where sigma(x) is out activation function and (t1 - o1) is the loss\n",
    "    = sigma(x) * (1 - sigma(x)) * 0.61\n",
    "    = 0.39 * (1 - 0.39) * 0.61, because sigma(x) is the value we received at output o1\n",
    "    ~ 0.1451\n",
    "\n",
    "    Now, we update the weights to the output layer by incorporating the error:\n",
    "    w <- w + Delta is the new weight\n",
    "    wo1x0' = wo1x0 + delta of wo1x0 = −1 + learning rate * Delta at output node * x0 = -1 + 0.5 * 0.1451 * 1 = -0.92\n",
    "    wo1h1' = wo1h1 + delta of wo1h1 = 1 + learning rate * Delta at output node * h1 = 1 + 0.5 * 0.1451 * 0.27 = 1.0195\n",
    "    wo1h2' = wo1h2 + delta of wo1h2 = 0.5 + learning rate * Delta at output node * h2 = 0.5 + 0.5 * 0.1451 * 0.62 = 0.5449\n",
    "\n",
    "    The error calculation at hidden layers = derivation of activation function * summation of wk,h * delta_k where k belongs to OUT as shown in class slides\n",
    "    for h1 - \n",
    "    delta_h1 = h1(1-h1) * Delta_o1 * wo1h1 = 0.27(1-0.27) * 0.1451 * 1 ~ 0.028\n",
    "\n",
    "    for h2 - \n",
    "    delta_h2 = h2(1-h2) * Delta_o1 * wo1h2 = 0.62(1-0.62) * 0.1451 * 0.5 ~ 0.0171\n",
    "\n",
    "    Now, we update the weights for hidden layers by incorporating the error:\n",
    "\n",
    "    wh1x0' = wh1x0 + delta = 1.5 + learning rate * Delta at h1 node * x0 = 1.5 + 0.5 * 0.028 * 1 = 1.514\n",
    "    wh1x1' = wh1x1 + delta = -2.5 + learning rate * Delta at h1 node * x1 = -2.5 + 0.5 * 0.028 * 1 = -2.486\n",
    "    wh1x2' = wh1x2 + delta = 1 + learning rate * Delta at h1 node * x2 = 1 + 0.5 * 0.028 * 0 = 1 + 0 = 1 \n",
    "\n",
    "    wh2x0' = wh2x0 + delta = 2 + learning rate * Delta at h2 node * x0 = 2 + 0.5 * 0.0171 * 1 = 2.00855\n",
    "    wh2x1' = wh2x1 + delta = -1.5 + learning rate * Delta at h2 node * x1 = -1.5 + 0.5 * 0.0171 * 1 = -1.49145\n",
    "    wh2x2' = wh2x2 + delta = 3 + learning rate * Delta at h2 node * x2 = 3 + 0.5 * 0.0171 * 0 = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515cfba",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Training a neural network\n",
    "\n",
    "We're now going to get past the pesky XOR problem that have been the bane of\n",
    "neural networks' existence.\n",
    "\n",
    "First, let's install libraries. We will be using the PyTorch package to train out neural network. You can use\n",
    "`pip` or `conda` to install the `torch` package.\n",
    "\n",
    "Let import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f733fe82-8d3a-4bc5-a5c3-1297760922f6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979fa120-9dc3-4bd7-9027-84996715212b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Then let's set up the network architecture. We are constructing the same model as the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece5cbd5-f3a1-405e-81ce-c0b6a1cd277c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    # connecting the input to the hidden layer\n",
    "    torch.nn.Linear(2, 2), # takes an vector of 2 numbers and outputs a vector of 2 numbers\n",
    "    torch.nn.Sigmoid(), # activation function\n",
    "    # connecting the hidden layer to the output layer\n",
    "    torch.nn.Linear(2, 1), # takes an vector of 2 numbers and outputs a vector of 1 number\n",
    "    torch.nn.Sigmoid() # activation function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e0108-b9a8-4155-a147-4fe58088633d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we need to train the network. Note that we are going to use the binary\n",
    "cross entropy loss function to optimally train this tiny network. This is a\n",
    "loss (error) measure we did not cover in class. We are using it for the\n",
    "assignment because, as its name suggests, it is well-suited for learning binary\n",
    "functions. XOR is of course such a function, having inputs and outputs that are\n",
    "1s and 0s, not continuous quantities.\n",
    "It's best for training if the examples presented to the network are random. So we are going to randomly generate XOR examples in each loop.\n",
    "\n",
    "PyTorch helps us out with automatic differentiation. We just need to use the gradient calculated with the loss function to modify the value of the model parameters (weights).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9aaf3035-09e1-47c5-8cfe-5877eb2365ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "def random_binary():\n",
    "    return random.choice([0, 1])\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "for i in range(5000):\n",
    "\n",
    "    # we're creating a random input and output tensor (vector) that follows the\n",
    "    # xor rule\n",
    "    x1, x2 = random_binary(), random_binary()\n",
    "    true_output = torch.Tensor([x1 ^ x2])\n",
    "\n",
    "    # getting the model's predicted output\n",
    "    predicted_output = model(torch.Tensor([x1, x2]))\n",
    "\n",
    "    # calculating the loss (error)\n",
    "    loss = loss_fn(predicted_output, true_output)\n",
    "\n",
    "\n",
    "    # calculating the gradients, in order to update the weights\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad(): # prevents gradient tracking during updates\n",
    "        # for each parameter (weight and bias) in the model\n",
    "        for param in model.parameters():\n",
    "            # update the parameter using the gradient scaled by the learning\n",
    "            # rate\n",
    "            param -= learning_rate * param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6803e64-fc25-40e0-96fa-71a171d3f915",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can now see what the neural network outputs for a given input. We are asking\n",
    "the model to predict $\\text{XOR}(0, 1)$ and $\\text{XOR}(1, 1)$. For your\n",
    "submission, please print the output for all input combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fb49b446-6e95-4a12-965a-191c24cfc7d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9999], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.0001], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9999], grad_fn=<SigmoidBackward0>)\n",
      "tensor([7.1151e-05], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.Tensor([0, 1])))\n",
    "print(model(torch.Tensor([1, 1])))\n",
    "print(model(torch.Tensor([1, 0])))\n",
    "print(model(torch.Tensor([0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848cab9-548b-41a1-be86-38728b05b0aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "If the neural network is not successful at XOR, try retraining it from scratch or increasing the number of epochs (training iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef38838a-89d1-4eb3-8477-cee01486aab5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prying open a neural network\n",
    "Now that we have a trained network, we can look at the weights and biases for each layer of the network. To do so, use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51275628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99986144e-01 9.80945246e-01]\n",
      " [9.76051274e-01 7.29749419e-03]\n",
      " [9.76062959e-01 7.30691774e-03]\n",
      " [2.25084248e-02 1.05107086e-06]]\n",
      "[[7.11464145e-05]\n",
      " [9.99928521e-01]\n",
      " [9.99928524e-01]\n",
      " [1.09427006e-04]]\n",
      "Input: (0, 0), XOR Output: 0\n",
      "Input: (0, 1), XOR Output: 1\n",
      "Input: (1, 0), XOR Output: 1\n",
      "Input: (1, 1), XOR Output: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "W0 = np.array([[-7.4787, -7.4792], [-8.8528, -8.8541]])\n",
    "b0 = np.array([11.1868, 3.9412])\n",
    "W2 = np.array([[19.7294, -20.0986]])\n",
    "b2 = np.array([-9.5642])\n",
    "\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "def predict(inputs):\n",
    "    layer0_output = np.dot(inputs, W0.T) + b0\n",
    "    layer0_output = sigmoid(layer0_output)\n",
    "    print(layer0_output)\n",
    "    layer2_output = np.dot(layer0_output, W2.T) + b2\n",
    "    final_output = sigmoid(layer2_output)\n",
    "    print(final_output)\n",
    "    return final_output\n",
    "\n",
    "predictions = predict(inputs)\n",
    "rounded_predictions = np.round(predictions)\n",
    "\n",
    "for i, (input_combination, prediction) in enumerate(zip(inputs, rounded_predictions)):\n",
    "    print(f\"Input: {tuple(input_combination)}, XOR Output: {int(prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c460a30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999861442999742\n",
      "0.9809452456394838\n",
      "7.114641454479416e-05\n",
      "0.9760512741967798\n",
      "0.0072974941917869586\n",
      "0.9999285210869288\n",
      "0.9760629590074197\n",
      "0.007306917739368373\n",
      "0.9999285240268854\n",
      "0.02250842476520902\n",
      "1.0510708555512887e-06\n",
      "0.00010942700604166475\n",
      "Input: (0, 0) -> XOR Output: 0.0001\n",
      "Input: (0, 1) -> XOR Output: 0.9999\n",
      "Input: (1, 0) -> XOR Output: 0.9999\n",
      "Input: (1, 1) -> XOR Output: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# ----- HELPER FUNCTION BY STUDENT ---------\n",
    "import numpy as np\n",
    "import math as Math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def predict(x1, x2):\n",
    "    x0 = 1\n",
    "    z_h1 = w_h1_x0 * x0 + w_h1_x1 * x1 + w_h1_x2 * x2\n",
    "    h1 = sigmoid(z_h1)\n",
    "    print(h1)\n",
    "\n",
    "    z_h2 = w_h2_x0 * x0 + w_h2_x1 * x1 + w_h2_x2 * x2\n",
    "    h2 = sigmoid(z_h2)\n",
    "    print(h2)\n",
    "    z_o = w_o_0 * x0 + w_o_1 * h1 + w_o_2 * h2\n",
    "    o = sigmoid(z_o)\n",
    "    print(o)\n",
    "    return o\n",
    "\n",
    "# Layer 0.weight\n",
    "w_h1_x1 = -7.4787\n",
    "w_h1_x2 = -7.4792\n",
    "# Layer 0.bias\n",
    "w_h2_x1 = -8.8528\n",
    "w_h2_x2 = -8.8541\n",
    "# Layer 2.weight\n",
    "w_h1_x0 = 11.1868\n",
    "w_h2_x0 = 3.9412\n",
    "# Layer 2.bias\n",
    "w_o_0 = -9.5642\n",
    "w_o_1 = 19.7294\n",
    "w_o_2 = -20.0986\n",
    "\n",
    "inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "predictions = [(x1, x2, predict(x1, x2)) for x1, x2 in inputs]\n",
    "\n",
    "for x1, x2, output in predictions:\n",
    "    print(f\"Input: ({x1}, {x2}) -> XOR Output: {output:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c2c1663-2016-4e21-835d-6381513b3546",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0.weight', Parameter containing:\n",
      "tensor([[-7.4787, -7.4792],\n",
      "        [-8.8528, -8.8541]], requires_grad=True)), ('0.bias', Parameter containing:\n",
      "tensor([11.1868,  3.9412], requires_grad=True)), ('2.weight', Parameter containing:\n",
      "tensor([[ 19.7294, -20.0986]], requires_grad=True)), ('2.bias', Parameter containing:\n",
      "tensor([-9.5642], requires_grad=True))]\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf066d8-17db-4105-a447-422ed5f7c326",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here's some sample output \n",
    "\n",
    "```python\n",
    "[('0.weight',\n",
    "  Parameter containing:\n",
    "  tensor([[-6.2839, -6.2317],\n",
    "          [-8.0318, -8.0085]], requires_grad=True)),\n",
    " ('0.bias',\n",
    "  Parameter containing:\n",
    "  tensor([9.3037, 3.4865], requires_grad=True)),\n",
    " ('2.weight',\n",
    "  Parameter containing:\n",
    "  tensor([[ 14.3675, -14.7218]], requires_grad=True)),\n",
    " ('2.bias',\n",
    "  Parameter containing:\n",
    "  tensor([-6.8876], requires_grad=True))]\n",
    "```\n",
    "\n",
    "Note that each of the 2 layers have 2 items in the output. Weights and biases.\n",
    "\n",
    "The weights are represented as a transformation matrix transforming the input\n",
    "to a layer.\n",
    "\n",
    "For example, for the Python output above, the weight connecting\n",
    "$x_1$ to $h_1$ has the value $-6.2839$, and the weight connecting $x_2$\n",
    "to $h_1$ has the value $-6.2317$. The bias value for $h_1$ is $9.3037$, and the bias value for $h_2$ is $3.4865$.\n",
    "\n",
    "## Questions\n",
    "\n",
    "* What were the final weights and biases for your network? How well did it do?\n",
    "* Using the weights and biases for your network, calculate the\n",
    "predictions for each for the four input-output combinations in the XOR\n",
    "truth table (you can find this in the slides or write it down\n",
    "yourself). You may do this manually or using code (looping, making\n",
    "functions, etc.), but do not use the trained model to do so.\n",
    "\n",
    "  To make this easier, here is a function that calculates the sigmoid\n",
    "  of an input vector.\n",
    "\n",
    "  ```python\n",
    "  import numpy as np\n",
    "\n",
    "  def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "  ```\n",
    "\n",
    "* Conduct an analysis of the weights and biases of the network you have just trained. Why does it compute the XOR function? See if you can intepret the neural networks as a combination of simpler logical functions such as OR and AND, or outline a different insight regarding the behavior of the network. Be creative and extensive here!\n",
    "\n",
    "## Credits\n",
    "\n",
    "Part 1 was adapted from University of Wisconsin-Madison's CS 540 class.\n",
    "\n",
    "## Submission guidelines\n",
    "\n",
    "Please submit the `.ipynb` file to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5fd16d",
   "metadata": {},
   "source": [
    "## Answers\n",
    "1. Below is the output after tweaking the epoch count - \n",
    "\n",
    "[('0.weight', Parameter containing:\n",
    "tensor([[-7.4787, -7.4792], [-8.8528, -8.8541]], requires_grad=True)), \n",
    "\n",
    "('0.bias', Parameter containing:\n",
    "tensor([11.1868,  3.9412], requires_grad=True)), \n",
    "\n",
    "('2.weight', Parameter containing:\n",
    "tensor([[ 19.7294, -20.0986]], requires_grad=True)), \n",
    "\n",
    "('2.bias', Parameter containing:\n",
    "tensor([-9.5642], requires_grad=True))]\n",
    "\n",
    "Model - \n",
    "Sequential(\n",
    "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "  (1): Sigmoid()\n",
    "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
    "  (3): Sigmoid()\n",
    ")\n",
    "\n",
    "For the Python output above, the weights are - \n",
    "- Layer 0.weight\n",
    "w_h1_x1 = -7.4787\n",
    "w_h1_x2 = -7.4792\n",
    "\n",
    "- Layer 0.bias\n",
    "w_h2_x1 = -8.8528\n",
    "w_h2_x2 = -8.8541\n",
    "\n",
    "- Layer 2.weight\n",
    "w_h1_x0 = 11.1868\n",
    "w_h2_x0 = 3.9412\n",
    "\n",
    "- Layer 2.bias\n",
    "w_o_0 = -9.5642\n",
    "w_o_1 = 19.7294\n",
    "w_o_2 = -20.0986\n",
    "\n",
    "The model seems to have performed well. \n",
    "The number of epoch was increased from the default value and i could see an increase from 0.9995 accuracy to almost 0.9999 for the xor (0, 1) case which is 1 as our target value.\n",
    "In the other case of XOR (1, 1), the answer we expect is 0 but after 5000 epoch training, we get as close as = 0.0001. After trying to increase the epoch size to even double, these values do not seem to increase.\n",
    "\n",
    "2. Getting values with sigmoid activation function for XOR with 0 and 1 values as in traditionally\n",
    "Input: (0 0) -> XOR Output: 0, expected value - 0\n",
    "Input: (0, 1) -> XOR Output: 1, expected value - 1\n",
    "Input: (1, 0) -> XOR Output: 1, expected value - 1\n",
    "Input: (1, 1) -> XOR Output: 0, expected value - 0\n",
    "\n",
    "Helper function added to check the values - \n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "W0 = np.array([[-7.4787, -7.4792], [-8.8528, -8.8541]])\n",
    "b0 = np.array([11.1868, 3.9412])\n",
    "W2 = np.array([[19.7294, -20.0986]])\n",
    "b2 = np.array([-9.5642])\n",
    "\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "def predict(inputs):\n",
    "    layer0_output = np.dot(inputs, W0.T) + b0\n",
    "    layer0_output = sigmoid(layer0_output)\n",
    "    layer2_output = np.dot(layer0_output, W2.T) + b2\n",
    "    final_output = sigmoid(layer2_output)\n",
    "    return final_output\n",
    "\n",
    "predictions = predict(inputs)\n",
    "rounded_predictions = np.round(predictions)\n",
    "\n",
    "for i, (input_combination, prediction) in enumerate(zip(inputs, rounded_predictions)):\n",
    "    print(f\"Input: {tuple(input_combination)}, XOR Output: {int(prediction)}\")\n",
    "\n",
    "3. The weights are as mentioned in the above questions - \n",
    "W0 = np.array([[-7.4787, -7.4792], [-8.8528, -8.8541]])\n",
    "b0 = np.array([11.1868, 3.9412])\n",
    "W2 = np.array([[19.7294, -20.0986]])\n",
    "b2 = np.array([-9.5642])\n",
    "\n",
    "The weights of layer 0 are negative, this means, the neurons will only be activated with certain inputs of x1 and x2. The first hidden layer h1 can be seen as implementing \n",
    "something similar to the NOT AND function, since it will output true for all combinations except when both inputs are true. The activation function in turn just helps standardize the output values between 0 and 1 for better understanding.\n",
    "\n",
    "in this case, if we see the layer W2's weights, we see one that one is very positive and one is very negative, this is helping our complex XOR function\n",
    "be sensitive to different cases. Because of this, XOR calculations of different values ends up being penalized and 0 whereas same values is not penalised.\n",
    "Additionally, in the cases of same values, like 0, 0, we see the calculations of h1 and h2 and o are like these\n",
    "h1 - 0.9999861442999742, h2 - 0.9809452456394838, o - 7.114641454479416e-05. Whereas, in the case of different numbers like - 0, 1, we see the values are like - \n",
    "h1 - 0.9760512741967798, h2 - 0.0072974941917869586, o - 0.9999285210869288. The difference is, same values end up in high h1 and h2 and different values\n",
    "end up in different h1 and h2, based on the weights we give to h1 and h2, the layer b0, biases for h1 and h2, h1 is more preferred and hence output is higher.\n",
    "\n",
    "We can indeed interpret it as a OR AND functions if we see the output layer which is a weighted sum layer followed by a transformation. \n",
    "In case of different values, we can see the XOR behaves similar to the OR function, if one of the output of hidden layer is closer to 1, the output is closer to 1.\n",
    "This is the case for 0, 1 - h1 - 0.9760512741967798, h2 - 0.0072974941917869586, o - 0.9999285210869288. 0 and 1 both values contribute to the values h1 and h2\n",
    "the weights of layer 0 and the bias term for h1 outputs a number closer to 1 whereas the same output a number closer to 0.\n",
    "\n",
    "An example quoted from Mitchell's neural nets book - \"If we assume boolean values of 1 (true) and -1 (false), then one way to\n",
    "use a two-input perceptron to implement the AND function is to set the weights wo = -8, and wl = wz = .5 and to determine it as an Or function, \n",
    "we alter wo = -.3.\" We see, the weights for our layer w2 are similar to this example, 19.7294, -20.0986. So, this neural net behaves indeed like an AND and OR function.\n",
    "Essentially, the second layer combines the outputs of the previous layer, emphasizes the effect of differing inputs and marginalizes outputs when both inputs are the same.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "name": "assignment-4-neural-networks.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
