HML HW 8 Assignment answers


* Submit a visualization of your MDS embeddings for both the human and GloVe data. There should be 10 images, 5 for the human ratings and 5 for the GloVe word embeddings.
  Ans - All the 10 images, 5 for human ratings and 5 for GloVe word embeddings can be found above.
* Comment on the MDS solutions generated from the human data. Does the ordinal arrangement of the adjectives in each set make sense, i.e., are they correctly ordered from "least" to "most"? (Or from "most" to "least" -- MDS is agnostic on left-right ordering in the solutions it generates.) Do the distances between the words follow the functional form you were expecting? Describe the patterns and provide hypotheses that might explain them.
  Ans - From the human data, we can see there are some patterns to note, like in the "anger" section, the distance between words similar to "anger" like "furious" and "annoyed" is less than it is between words like "anger" and "calm". Similarly for "danger", distance between "danger" and "safe" is the biggest and "danger" and "deadly" is small. On the other hand, words like "certain" and "probable" should be kinda far from each other which they are, but "probable" and "possible" should be nearby which is not the case. "most" and "none" seem like opposites so the distance between them if the most and "most" and "all" seem like more similar so the distance between them is smaller comparatively. The distances between the function form mostly follows the form that i was expecting but in the case of "probability", "rare" seems like the opposite so that is fine. The concern is why is "certain" closer to "probable" than "possible" maybe because maybe as in natural language, "certain" and "probable" often appear in similar contexts, such as in discussions about outcomes with a positive expectation like "it is certain or probable to happen" wheras "Possible" is a looser word which may appear in a wider range of contexts, which could lead to a larger distance from "certain" in embedding spaces.
* Consider the same questions as in (2) for GloVe's MDS solutions.
  Ans - The GloVe's MDS solution for all the 5 words is very interesting, it seems like a little unexpected in some cases like the "probable" case from human data question, i see in GloVe, distance between "probable" and "certain" is more than the distance between "certain" and "possible" is. Which is not intuitive at all, certain does not sound similar to possible. The chart for "great" is well great, the distance between great, best, and good are almost equal but "fair" is somewhere in the corner which suggests a great divison. This still does not compare to human data where "great" and "best" are the nearest and "good" is a good amount amount and "fair" is even further. Quantity part is similar to humans and Anger part is also similar with a lower stress value. The most surprising so far has been "danger", the distance between "sketchy" and "dangerous" is more than distance between "sketchy" and "deadly". The distance between "danger" and "safe" is surprisingly small maybe because they are direct antonyms and the word pairing has appeared in natural language text more often than the word pair "sketchy" and "dangerous". Like in these sentences we can see that effect, "In times of danger, resort to safety" or "It is a sketchy neighborhood".
* Now compare the MDS solutions for the human participants and for
  GloVe. What differences do you notice? What does the model capture of human's word representations, and what does it miss? Provide hypotheses for why it succeeds and why it fails.
  Ans - As mentioned in the previous answers, Anger, greatness, quantity are the embeddings that the GloVe models provides similar outputs to humans because concepts that are relatively concrete like "quantity" or have well-defined positive or negative associations like "anger" tend to form distinct clusters in embedding space. This makes them easier for the model - GloVe to model in a way that aligns with human judgment. On the other hand, for probability and danger, we see the human data captures a more accurate representations and the GloVe model seems to be confused that may be because the terms are a little abstract like "probability" and more context dependent. GloVe does not consider sentence-level context in embeddings, so it may fail to differentiate between these context-specific uses.Probably, this can result in "probability" or "danger" terms clustering in ways that donâ€™t align with human intuition. 
* What is the **stress** statistic that is outputted for each MDS
  solution? What does this statistic represent? What are "acceptable" ranges for it, and what values suggest that more dimensions may be needed to adequately capture the distances/similarities?
  Ans - The stress value of mds represents how well the model handles being reduced from one dimension to another, it can be considered as a "distortion" value. Lower stress value as one might anticipate meaning the MDS projection accurately represents the original distances. High stress values suggest that the transformation to a lower dimension is not exactly an accurate representation of the embedding. Values below 0.1 are considered good, greater than 0.2 or 0.3 is where we see the significant distortion and we say that the model fit is poor.

  For the next part - 
# Choosing the 3rd option - 
# Compare the output from Glove models of differing sizes. 
# Do larger models do a better job of getting the word order right? 
# Are they more similar to humans?

# Code for the glove.42B.300d.txt in the notebook

### Comparisons between Glove 6B, 42B, and 820B datasets and if they come any closer to Human :
Surprisingly, the larger models do not significantly improve in the distance parameter between the values and the order of the words remains the same. One thing to note is that the stress levels significantly reduce as the models go bigger and bigger. For example, the stress levels for Anger word embedding for GloVe 6B model is 0.26 and the one for GloVe 840B model is 0.15 (this is for abstract embeddings). For stringent embeddings like quantity, we can see the stress level reduces from 6B model (0.157) to 42B model (0.06) and to 840B model (0.093). Another thing to note here is that in terms of stress levels, 42B models performs better than 840B models in all embeddings except Anger. Last thing to note here is that, as compared to human data, we can see these large models are still a little far off but in one particular embedding, we can see that they seem to match human data closely - greatness. In GloVe 6B model, we can see big distance between great and best, human data shows great and best closer by. The larger models 42B and 840B have the distance between great and best almost as much as the human data. So, in conclusion, larger models shows some improvements and possibility of emulating human data but they are not quiet there yet.