{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5076937c",
   "metadata": {},
   "source": [
    "# Assignment 3: Vector Representations & LSA\n",
    "\n",
    "**Please do not consult external resources for this assignment.**\n",
    "\n",
    "Make sure you have done the required reading for this homework (which was also required reading for class):\n",
    "\n",
    "- [Jurafsky, D., & Martin, J. H. (2020). Speech and Language Processing, Chapter 6: Vector Semantics and Embeddings (forthcoming 3rd edition). Prentice-Hall. [pp. 1-11]](https://web.stanford.edu/~jurafsky/slp3/6.pdf)\n",
    "\n",
    "In this assignment, we are going to use a pretrained LSA model. Specifically we will be using the `EN_100k_lsa model` by Fritz Günther [1]. It will be useful for this assignment to read more about the model and understand the data it was trained on. The model is large, so you might have to spend some time downloading it.\n",
    "\n",
    "[1] [Günther, F., Dudschig, C., & Kaup, B. (2015). LSAfun – An R package for computations based on Latent Semantic Analysis. Behavior Research Methods, 47, 930-944.](https://sites.google.com/site/fritzgntr/software-resources/semantic_spaces)\n",
    "\n",
    "## Submission guidelines\n",
    "\n",
    "Please upload your Jupyter notebook to the Canvas Assignment. Please do not include any system specific configuration, such as the installation of dependencies, as code in the notebook (you can comment it out).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c2586",
   "metadata": {},
   "source": [
    "## Retrieving the LSA Model\n",
    "\n",
    "The `EN_100k_lsa` dataset we're using for this assignment is distributed for the R programming language by the author. We have converted it to the text format that's typically used for the vector representations, and that's what we will use for this assignment. You will need a Georgia Tech account to download it from [here.](https://gatech.instructure.com/files/54010573/download?download_frd=1)\n",
    "\n",
    "In the file, each line is a quoted word, followed by 300 floating points numbers that constitute the vector representation of that vector. Here are a few examples from the file:\n",
    "\n",
    "```\n",
    "\"is\" 38.576145560208 21.6394607567694 19.7566635160741 -10.8594517804468 7.51268972210512 {295 others} \n",
    "\"was\" 69.1618505390472 -34.1040373913203 41.9157366516055 -14.2414103748296 -5.62593149303477 {295 others} \n",
    "\"be\" 24.9892859457854 16.3246671002679 3.70758479872187 -12.302839896438 2.23646336543604 {295 others}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb97e45",
   "metadata": {},
   "source": [
    "## Parsing the File\n",
    "\n",
    "We have provided the code to parse the file into a dictionary of word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84d11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def parse_file(file_name):\n",
    "    word_vectors = {}\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            first_whitespace = line.index(\" \")\n",
    "            word = line[:first_whitespace].strip('\"')\n",
    "            vector = np.array(line[first_whitespace + 1:].split(\" \"), dtype=np.float32)\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc4f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_word_vectors = parse_file(\"files/EN_100k_lsa.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0178abc-a7cf-4523-94a7-557ac885c1ba",
   "metadata": {},
   "source": [
    "For example, use `dict_word_vectors[\"is\"]` to access the word vector for the word \"is\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf830bd-e308-4e4f-ab05-37d1711543a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.21761322e+02, -7.53212280e+01,  1.96545593e+02,  1.54491928e+02,\n",
       "        5.50510178e+01, -2.55067123e+02,  8.02171478e+01, -8.99195480e+01,\n",
       "        2.91481934e+02,  3.79438019e+02,  9.17247925e+01, -1.30209885e+02,\n",
       "        1.65959106e+02, -2.96785278e+02,  4.34712563e+01, -5.82672806e+01,\n",
       "        1.69251709e+01, -8.79788113e+00, -1.97886444e+02, -9.18110809e+01,\n",
       "       -8.26303329e+01,  9.52104874e+01,  2.03846970e+02, -2.85616882e+02,\n",
       "        1.03230957e+02, -2.49806076e+02,  1.70082809e+02, -1.14347286e+01,\n",
       "        7.39332123e+01,  4.62236595e+01, -6.16472321e+01,  1.24699478e+01,\n",
       "        1.16835470e+01,  2.07751816e+02,  3.11458569e+01, -2.89486053e+02,\n",
       "        1.32502594e+02, -1.45918457e+02,  1.14770508e+01, -8.10860901e+01,\n",
       "       -4.93820839e+01,  2.18805885e+01,  9.85299587e+00, -1.80958881e+01,\n",
       "       -2.29861088e+01,  1.07338896e+01,  1.49447527e+01, -2.02187519e+01,\n",
       "       -7.29695587e+01,  5.11481285e+00,  1.07671593e+02,  1.54620132e+02,\n",
       "       -4.15899353e+01, -3.29246864e+01,  2.42134762e+01, -1.58593506e+02,\n",
       "       -2.22252140e+01,  2.00554840e+02,  1.23903130e+02,  7.11942368e+01,\n",
       "        1.63305302e+01,  3.30330925e+01,  3.88467178e+01,  9.32883224e+01,\n",
       "       -1.56548370e+02, -5.23178406e+01, -1.22340057e+02,  5.19962997e+01,\n",
       "       -9.64696121e+01, -6.13723640e+01, -1.48426542e+01,  1.99836521e+01,\n",
       "        5.10410461e+01,  1.56960541e+02, -1.28912308e+02,  8.04247761e+00,\n",
       "        5.07138672e+01,  1.24348663e+02,  1.25236616e+01,  5.01964531e+01,\n",
       "       -3.55957842e+00,  5.00599174e+01, -7.73813095e+01, -9.42171574e+00,\n",
       "       -6.94446640e+01,  8.47514191e+01, -1.20144024e+01, -2.71516705e+01,\n",
       "       -6.86968765e+01, -1.87692127e+01,  2.70077682e+00,  6.53909683e+01,\n",
       "       -4.76674194e+01,  1.76344609e+00, -5.53242731e+00,  1.57835913e+01,\n",
       "       -1.80470037e+00,  5.59592972e+01, -7.03714066e+01,  2.38783169e+01,\n",
       "       -6.66909485e+01, -1.08523232e+02,  1.54646683e+01, -8.51064987e+01,\n",
       "       -5.16822815e+01, -1.12654085e+01, -1.42975206e+01,  3.21771049e+01,\n",
       "        3.04234123e+01,  1.01255463e+02,  1.52218580e+01,  7.83797073e+00,\n",
       "       -6.34613562e+00,  3.47938752e+00, -8.24144669e+01, -4.01048431e+01,\n",
       "        5.52279043e+00, -4.95636940e+00, -6.65524979e+01, -1.37837493e+00,\n",
       "       -8.04165421e+01,  3.68863487e+01, -4.35126152e+01,  3.81347504e+01,\n",
       "        2.94965057e+01, -4.12069817e+01,  5.17108965e+00, -5.71794930e+01,\n",
       "       -6.73539639e+00, -8.56246662e+00,  1.66875153e+01, -4.25606842e+01,\n",
       "       -1.24365578e+01, -3.93315926e+01, -1.23718615e+01,  4.17997856e+01,\n",
       "       -5.68671761e+01,  3.75771561e+01, -6.48152542e+01,  7.64124985e+01,\n",
       "        4.34923630e+01,  3.50819969e+01, -1.71950550e+01,  7.04054947e+01,\n",
       "       -8.50237579e+01,  4.74756317e+01, -5.49184952e+01,  4.93370171e+01,\n",
       "        2.23565750e+01, -7.30666046e+01,  1.25650234e+01,  9.32411804e+01,\n",
       "        4.47788620e+01, -7.24486237e+01,  3.42404866e+00,  3.09056034e+01,\n",
       "       -1.24153843e+01, -4.98756676e+01,  2.70259500e-01,  8.94159927e+01,\n",
       "        1.70943737e+01,  5.25754166e+01, -6.89797287e+01,  5.23212738e+01,\n",
       "        3.42328033e+01,  4.80749512e+01,  5.86468391e+01,  4.57410164e+01,\n",
       "        3.52796631e+01, -2.43733292e+01,  3.65926476e+01, -7.23248138e+01,\n",
       "       -7.24806976e+01, -2.87711358e+00,  1.23658419e+00, -3.18743477e+01,\n",
       "       -7.74946136e+01,  5.01676102e+01, -3.83441772e+01,  3.93197784e+01,\n",
       "        1.54295492e+01,  6.05264997e+00,  3.43030968e+01, -6.76958237e+01,\n",
       "        4.25689888e+00,  1.01460945e+02, -2.55385151e+01,  3.06462460e+01,\n",
       "       -7.43794708e+01, -8.56154156e+00, -2.81103992e+00, -1.09814726e-01,\n",
       "        2.56356354e+01,  2.47778358e+01, -1.43194609e+01,  3.81218452e+01,\n",
       "        2.87945232e+01,  1.98006363e+01,  4.10847664e+01,  1.97233653e+00,\n",
       "       -1.52646389e+01, -8.12338257e+00,  1.22310555e+02, -1.33361206e+01,\n",
       "        1.70869598e+01, -2.39808788e+01, -1.88810329e+01, -1.92235298e+01,\n",
       "       -5.11558151e+00,  3.00950966e+01, -6.61975555e+01,  5.20637245e+01,\n",
       "       -4.19560099e+00, -2.11865673e+01, -4.73056946e+01, -1.43625107e+01,\n",
       "        4.57606697e+00,  1.29163446e+01, -2.61899033e+01, -6.77873611e+00,\n",
       "        2.22982049e+00, -5.12296906e+01,  2.62323017e+01, -5.03585854e+01,\n",
       "       -4.98324356e+01,  3.61564827e+01,  1.45471087e+01,  6.38852358e+00,\n",
       "       -1.07532682e+01, -1.75046310e+01,  3.70387344e+01, -3.61918521e+00,\n",
       "        9.67028522e+00,  2.49656162e+01,  1.87101078e+01, -2.62976627e+01,\n",
       "        2.78587551e+01, -3.97704315e+00, -1.59660826e+01,  3.92094827e+00,\n",
       "        5.36890564e+01, -4.53794098e+01, -4.71164703e+00, -2.52250652e+01,\n",
       "       -7.83269272e+01, -1.87286968e+01, -4.29264259e+01,  4.60137978e+01,\n",
       "       -8.66704881e-01,  1.57162800e+01, -2.67486019e+01,  3.89376335e+01,\n",
       "       -2.85113163e+01, -1.31830816e+01, -1.19261980e+00,  6.82661972e+01,\n",
       "        2.03141060e+01, -1.17864561e+01,  7.80366564e+00,  3.74278212e+00,\n",
       "        3.04264784e+00,  6.86639190e-01, -4.82884216e+01,  4.54260290e-01,\n",
       "        1.32919703e+01, -2.00883746e+00,  1.43909626e+01, -1.81463356e+01,\n",
       "       -2.82388439e+01, -1.75879173e+01,  3.00483398e+01,  5.46565819e+00,\n",
       "       -1.12896881e+01,  1.79440804e+01, -8.12598515e+00,  8.04336739e+00,\n",
       "        1.14675941e+01,  9.63005638e+00,  4.73791552e+00, -3.75197830e+01,\n",
       "        1.84303570e+01, -3.12377453e+00,  4.05053139e+00, -7.61418819e+00,\n",
       "       -4.36917038e+01, -2.67078114e+01,  7.24925995e+00,  1.88295517e+01,\n",
       "       -3.86871159e-01, -5.01556730e+00, -6.46195555e+00,  6.41119623e+00,\n",
       "       -1.25362730e+01,  1.97368592e-01, -2.36084690e+01, -3.03419933e+01,\n",
       "       -2.62156353e+01, -6.11240530e+00,  1.79886150e+01,  2.76352143e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_word_vectors[\"is\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7e408",
   "metadata": {},
   "source": [
    "## Implementation Guidelines & Tips\n",
    "\n",
    "The following parts of the assignment should be analyzed and answered by computing similarities and or differences between two words. You may use the [`scipy.spatial.distance.cosine`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) function. Keep in mind that the function returns a **distance**. To find **similarity**, you can subtract the returned distance from 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e5e33",
   "metadata": {},
   "source": [
    "## TOEFL Synonym questions\n",
    "\n",
    "We're going to use the LSA model to complete TOEFL synonym questions. Let's see how it fares.\n",
    "\n",
    "[You can see an example question here.](https://aclweb.org/aclwiki/TOEFL_Synonym_Questions_(State_of_the_art))\n",
    "\n",
    "Use the `files/syntest.csv` dataset to complete this section.\n",
    "\n",
    "\n",
    "\n",
    "Please use the LSA word vectors to predict the answers for each of the 20 questions.  \n",
    "\n",
    "Print a table with columns \\['Question', 'Option1', 'Option2', 'Option3', 'Option4', 'Correct', 'Type', 'model_answers', 'accuracy'\\]\n",
    "\n",
    "Then answer the following questions.\n",
    "\n",
    "1. How well does the model do?\n",
    "2. What kind of items does the model do well on? What kind of items does the model fail on? Why does it fail on those items? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d488351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    return (1 - cosine(vector1, vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a719375",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# pass\n",
    "\n",
    "syntest = pd.read_csv(\"files/syntest.csv\")\n",
    "\n",
    "def predict_answers(syntest, dict_word_vectors):\n",
    "    predicted_answers = []\n",
    "    \n",
    "    for _, row in syntest.iterrows():\n",
    "        question = row['Question']\n",
    "        options = [row['Answer1'], row['Answer2'], row['Answer3'], row['Answer4']]\n",
    "        target_vector = dict_word_vectors[question]\n",
    "        max_similarity = float('-inf')\n",
    "        model_answer = \"\"\n",
    "        \n",
    "        for option in options:\n",
    "            if option in dict_word_vectors:\n",
    "                option_vector = dict_word_vectors[option]\n",
    "                similarity = compute_cosine_similarity(target_vector, option_vector)\n",
    "\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    model_answer = option\n",
    "            else:\n",
    "                print(\"Word check not found in the lSA database provided by TAs\")\n",
    "        \n",
    "        predicted_answers.append({\n",
    "            'Question': row['Question'], 'Answer1': row['Answer1'],'Answer2': row['Answer2'],\n",
    "            'Answer3': row['Answer3'],'Answer4': row['Answer4'],'Correct': row['Correct'],\n",
    "            'Type': row['Type'],'model_answers': model_answer,'accuracy': \"100%\" if model_answer == row['Correct'] else \"0%\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(predicted_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6dc3698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Question     Answer1      Answer2    Answer3     Answer4    Correct  \\\n",
      "0         large        wide      massive       tall         far       tall   \n",
      "1          near       small        close       open     similar      close   \n",
      "2         enjoy  appreciate    celebrate       like       claim       like   \n",
      "3         lucky   fortunate        happy     tricky      sneaky  fortunate   \n",
      "4        pretty      bright       joyful    popular   beautiful  beautiful   \n",
      "5        street      gutter         road    railway    building       road   \n",
      "6       apology       guilt  forgiveness     excuse       grief     excuse   \n",
      "7         quick         shy        hasty      small        fast       fast   \n",
      "8           sad   desperate        angry    unhappy   disgusted    unhappy   \n",
      "9   intelligent    educated        smart     active  successful      smart   \n",
      "10        night        dawn          day        sky        dusk        day   \n",
      "11       friend      father        enemy  colleague     partner      enemy   \n",
      "12        white       black          red   colorful       green      black   \n",
      "13         life    hospital        death       pain       blood      death   \n",
      "14         rich   beautiful       famous       poor      polite       poor   \n",
      "15         hope         joy      despair  happiness       doubt    despair   \n",
      "16          old     ancient         dead      grown       young      young   \n",
      "17     remember       think       forget       know       laugh     forget   \n",
      "18         fast        huge        rapid       slow      broken       slow   \n",
      "19       famous     unknown      beloved  competent   luxurious    unknown   \n",
      "\n",
      "       Type model_answers accuracy  \n",
      "0   synonym       massive       0%  \n",
      "1   synonym         close     100%  \n",
      "2   synonym    appreciate       0%  \n",
      "3   synonym         happy       0%  \n",
      "4   synonym     beautiful     100%  \n",
      "5   synonym          road     100%  \n",
      "6   synonym        excuse     100%  \n",
      "7   synonym          fast     100%  \n",
      "8   synonym         angry       0%  \n",
      "9   synonym         smart     100%  \n",
      "10  antonym           day     100%  \n",
      "11  antonym        father       0%  \n",
      "12  antonym         black     100%  \n",
      "13  antonym         death     100%  \n",
      "14  antonym          poor     100%  \n",
      "15  antonym         doubt       0%  \n",
      "16  antonym         young     100%  \n",
      "17  antonym        forget     100%  \n",
      "18  antonym          slow     100%  \n",
      "19  antonym       beloved       0%  \n"
     ]
    }
   ],
   "source": [
    "print(predict_answers(syntest, dict_word_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c73b62",
   "metadata": {},
   "source": [
    "## Part 2: SAT analogy questions\n",
    "\n",
    "We're now going to use LSA to complete SAT analogy questions (which were discontinued after the year 2005).\n",
    "\n",
    "[You can find an example question here.](https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art))\n",
    "\n",
    "Pick 5 analogy questions from the SAT practice test book that can be [found here.](https://dbgyan.files.wordpress.com/2013/02/501_word_analogy.pdf) Pick items that you think you will find interesting or useful to discuss when answering the questions.\n",
    "\n",
    "Use the LSA model to predict answers on the example question, and your 5 selected questions. Here's one way to perform a comparison: To represent a word pair, you can add the two word vectors together. You can then compare the word pair vectors to each other to answer the questions. However, you can take other paths if you think they will be interesting to discuss.\n",
    "\n",
    "Then answer the following questions:\n",
    "\n",
    "3. Why did you pick the items you picked? How well did model do on them?\n",
    "4. What kind of items does the model do well on? What kind of items does the model fail on? Why does it fail on those items? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1546554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_analogy(Question, dict_word_vectors):\n",
    "    predicted_answers = []\n",
    "    \n",
    "    question = Question['Question'][2]\n",
    "    vec1 = dict_word_vectors[Question['Question'][0]]\n",
    "    vec2 = dict_word_vectors[Question['Question'][1]]\n",
    "    options = [Question['Answer1'], Question['Answer2'], Question['Answer3'], Question['Answer4']]\n",
    "    target_vector = dict_word_vectors[question]\n",
    "    analogy_vector = vec2 - vec1\n",
    "    max_similarity = float('-inf')\n",
    "    model_answer = \"\"\n",
    "    \n",
    "    for option in options:\n",
    "        if option in dict_word_vectors:\n",
    "            option_vector = target_vector - dict_word_vectors[option]\n",
    "            similarity = np.dot(analogy_vector, option_vector)\n",
    "\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                model_answer = option\n",
    "        else:\n",
    "            print(\"Word check not found in the lSA database provided by TAs\")\n",
    "    \n",
    "    predicted_answers.append({\n",
    "        'Question': Question['Question'], 'Answer1': Question['Answer1'],'Answer2': Question['Answer2'],\n",
    "        'Answer3': Question['Answer3'],'Answer4': Question['Answer4'],'Correct': Question['Correct'],\n",
    "        'model_answers': model_answer,'accuracy': 100 if model_answer == Question['Correct'] else 0\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(predicted_answers)\n",
    "    # return model_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4345fc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is:               Question Answer1 Answer2 Answer3  Answer4 Correct model_answers  \\\n",
      "0  [about, bout, mend]   amend    near    tear  dismiss   amend          near   \n",
      "\n",
      "   accuracy  \n",
      "0         0  \n",
      "The answer is:                  Question Answer1  Answer2 Answer3  Answer4  Correct  \\\n",
      "0  [warm, hot, hilarious]   humid  raucous  summer  amusing  amusing   \n",
      "\n",
      "  model_answers  accuracy  \n",
      "0       amusing       100  \n",
      "The answer is:                 Question Answer1 Answer2 Answer3 Answer4 Correct  \\\n",
      "0  [trail, grain, grail]   train    path   wheat    holy   train   \n",
      "\n",
      "  model_answers  accuracy  \n",
      "0          path         0  \n",
      "The answer is:                            Question Answer1 Answer2 Answer3  Answer4  Correct  \\\n",
      "0  [particular, fussy, subservient]    meek   above  cranky  uptight  uptight   \n",
      "\n",
      "  model_answers  accuracy  \n",
      "0         above         0  \n",
      "The answer is:                 Question Answer1 Answer2 Answer3 Answer4 Correct  \\\n",
      "0  [horse, board, train]  stable    shoe    ride   mount    shoe   \n",
      "\n",
      "  model_answers  accuracy  \n",
      "0          ride         0  \n",
      "The answer is:                    Question Answer1 Answer2 Answer3    Answer4 Correct  \\\n",
      "0  [son, nuclear, extended]  father  mother  cousin  daughters  cousin   \n",
      "\n",
      "  model_answers  accuracy  \n",
      "0        father         0  \n",
      "The answer is:                     Question Answer1 Answer2  Answer3 Answer4 Correct  \\\n",
      "0  [zenith, fear, composure]    apex  heaven  heights   nadir   nadir   \n",
      "\n",
      "  model_answers  accuracy  \n",
      "0         nadir       100  \n",
      "The answer is:                 Question Answer1 Answer2 Answer3 Answer4 Correct  \\\n",
      "0  [bog, slumber, sleep]   dream   foray   marsh   night   marsh   \n",
      "\n",
      "  model_answers  accuracy  \n",
      "0         marsh       100  \n"
     ]
    }
   ],
   "source": [
    "example_question = {'Question': ['about', 'bout', 'mend'],\n",
    "            'Answer1': 'amend',\n",
    "            'Answer2': 'near',\n",
    "            'Answer3': 'tear',\n",
    "            'Answer4': 'dismiss',\n",
    "            'Correct': 'amend'}\n",
    "\n",
    "predicted_word = solve_analogy(example_question, dict_word_vectors)\n",
    "print(\"The answer is:\",  predicted_word)\n",
    "\n",
    "example_question = {'Question': ['warm', 'hot', 'hilarious'],\n",
    "            'Answer1': 'humid',\n",
    "            'Answer2': 'raucous',\n",
    "            'Answer3': 'summer',\n",
    "            'Answer4': 'amusing',\n",
    "            'Correct': 'amusing'}\n",
    "\n",
    "predicted_word = solve_analogy(example_question, dict_word_vectors)\n",
    "print(\"The answer is:\",  predicted_word)\n",
    "\n",
    "\n",
    "question = {'Question': ['trail', 'grain', 'grail'],\n",
    "             'Answer1': 'train',\n",
    "             'Answer2': 'path',\n",
    "             'Answer3': 'wheat',\n",
    "             'Answer4': 'holy',\n",
    "             'Correct': 'train'}\n",
    "\n",
    "predicted_word = solve_analogy(question, dict_word_vectors)\n",
    "print(\"The answer is:\",  predicted_word)\n",
    "\n",
    "question = {'Question': ['particular', 'fussy', 'subservient'],\n",
    "             'Answer1': 'meek',\n",
    "             'Answer2': 'above',\n",
    "             'Answer3': 'cranky',\n",
    "             'Answer4': 'uptight',\n",
    "             'Correct': 'uptight'}\n",
    "\n",
    "\n",
    "predicted_word = solve_analogy(question, dict_word_vectors)\n",
    "print(\"The answer is:\",  predicted_word)\n",
    "\n",
    "question = {'Question': ['horse', 'board', 'train'],\n",
    "             'Answer1': 'stable',\n",
    "             'Answer2': 'shoe',\n",
    "             'Answer3': 'ride',\n",
    "             'Answer4': 'mount',\n",
    "             'Correct': 'shoe'}\n",
    "\n",
    "predicted_word = solve_analogy(question, dict_word_vectors)\n",
    "print(\"The answer is:\",  predicted_word)\n",
    "\n",
    "question = {'Question': ['son', 'nuclear', 'extended'],\n",
    "             'Answer1': 'father',\n",
    "             'Answer2': 'mother',\n",
    "             'Answer3': 'cousin',\n",
    "             'Answer4': 'daughters',\n",
    "             'Correct': 'cousin'}\n",
    "\n",
    "predicted_word = solve_analogy(question, dict_word_vectors)\n",
    "print(\"The answer is:\",  predicted_word)\n",
    "\n",
    "question = {'Question': ['zenith', 'fear', 'composure'],\n",
    "             'Answer1': 'apex',\n",
    "             'Answer2': 'heaven',\n",
    "             'Answer3': 'heights',\n",
    "             'Answer4': 'nadir',\n",
    "             'Correct': 'nadir'}\n",
    "\n",
    "predicted_word = solve_analogy(question, dict_word_vectors)\n",
    "print(\"The answer is:\",  predicted_word)\n",
    "\n",
    "question = {'Question': ['bog', 'slumber', 'sleep'],\n",
    "             'Answer1': 'dream',\n",
    "             'Answer2': 'foray',\n",
    "             'Answer3': 'marsh',\n",
    "             'Answer4': 'night',\n",
    "             'Correct': 'marsh'}\n",
    "\n",
    "predicted_word = solve_analogy(question, dict_word_vectors)\n",
    "print(\"The answer is:\",  predicted_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221956b",
   "metadata": {},
   "source": [
    "## Category Typicality\n",
    "\n",
    "We're now going to attempt to replicate human typicality ratings of the categories `color` and `flower` from [Castro et al. (2022)](https://psyarxiv.com/4gzn6/). Castro et al. (2022) asked participants to generate as many examples as they could for the the categories and used the data to calculate human typicality ratings.\n",
    "\n",
    "Retrieve the list of category types (responses) and typicality scores from `files/castro_et_al_typicality.csv` associated with `color` and `flower`.\n",
    "\n",
    "* Compute how similar each response for category color is to the \"color\" (the word) vector representation\n",
    "* Compute how similar each response for category flower is to the \"flower\" (the word) vector representation\n",
    "* Compare the results to the human data (typicality scores in the csv)\n",
    "\n",
    "This function might help with comparisons with human data: [`scipy.stats.spearmanr`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html?highlight=spearman).\n",
    "\n",
    "NOTE: If the model does not have a vector for a particular word, remove that word from the analysis.\n",
    "\n",
    "1. What kind of items does the model do well on? What kind of items does the model fail on? Why does it fail on those items? \n",
    "2. How well does the model do compared to human data? Why does the model have higher correlation with human data for one category and negative correlation for the other category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6512eb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Category   Response  Typicality_Scores\n",
      "178  A color       Blue               0.97\n",
      "179  A color        Red               0.96\n",
      "180  A color      Green               0.92\n",
      "181  A color     Yellow               0.85\n",
      "182  A color     Orange               0.79\n",
      "183  A color     Purple               0.77\n",
      "184  A color      Black               0.72\n",
      "185  A color      White               0.58\n",
      "186  A color       Pink               0.55\n",
      "187  A color      Brown               0.38\n",
      "188  A color       Gray               0.21\n",
      "189  A color     Violet               0.19\n",
      "190  A color    Magenta               0.13\n",
      "191  A color       Teal               0.11\n",
      "192  A color     Indigo               0.10\n",
      "193  A color        Tan               0.09\n",
      "194  A color  Turquoise               0.09\n",
      "195  A color     Silver               0.08\n",
      "196  A color   Lavender               0.07\n",
      "197  A color       Gold               0.06\n",
      "198  A color      Beige               0.06\n",
      "     Category   Response  Typicality_Scores\n",
      "450  A flower       Rose               0.89\n",
      "451  A flower      Tulip               0.53\n",
      "452  A flower      Daisy               0.50\n",
      "453  A flower       Lily               0.30\n",
      "454  A flower  Sunflower               0.26\n",
      "455  A flower   Daffodil               0.22\n",
      "456  A flower  Carnation               0.22\n",
      "457  A flower    Petunia               0.15\n",
      "458  A flower     Orchid               0.14\n",
      "459  A flower  Dandelion               0.12\n",
      "460  A flower      Pansy               0.11\n",
      "461  A flower     Violet               0.11\n",
      "462  A flower       Iris               0.08\n",
      "463  A flower      Lilac               0.08\n",
      "464  A flower      Peony               0.08\n",
      "465  A flower   Gardenia               0.07\n",
      "466  A flower   Marigold               0.07\n",
      "467  A flower        Mum               0.06\n",
      "468  A flower   Geranium               0.05\n",
      "469  A flower  Hydrangea               0.05\n",
      "[0.6581032276153564, 0.6298416256904602, 0.48608556389808655, 0.6681832075119019, 0.5926750302314758, 0.531714677810669, 0.6407472491264343, 0.5891935229301453, 0.5633962750434875, 0.5313782095909119, 0.5611620545387268, 0.5437813401222229, 0.7871934175491333, 0.007358688861131668, 0.5028117895126343, 0.4907846748828888, 0.37655746936798096, 0.25387972593307495, 0.26416951417922974, 0.24430836737155914, 0.5675936341285706, 0.4901319146156311, 0.7996804714202881, 0.7203385233879089, 0.7544000744819641, 0.377120703458786, 0.8940339684486389, 0.6455138921737671, 0.8390150666236877, 0.7330541610717773, 0.6957420110702515, 0.833514392375946, 0.6529996991157532, 0.658237874507904, 0.7534120678901672, 0.8279337882995605, 0.7706102132797241, 0.8231885433197021, 0.16554765403270721, 0.8775724768638611, 0.9194568991661072] ['Blue', 'Red', 'Green', 'Yellow', 'Orange', 'Purple', 'Black', 'White', 'Pink', 'Brown', 'Gray', 'Violet', 'Magenta', 'Teal', 'Indigo', 'Tan', 'Turquoise', 'Silver', 'Lavender', 'Gold', 'Beige', 'Rose', 'Tulip', 'Daisy', 'Lily', 'Sunflower', 'Daffodil', 'Carnation', 'Petunia', 'Orchid', 'Dandelion', 'Pansy', 'Violet', 'Iris', 'Lilac', 'Peony', 'Gardenia', 'Marigold', 'Mum', 'Geranium', 'Hydrangea']\n",
      "[0.6581032276153564, 0.6298416256904602, 0.48608556389808655, 0.6681832075119019, 0.5926750302314758, 0.531714677810669, 0.6407472491264343, 0.5891935229301453, 0.5633962750434875, 0.5313782095909119, 0.5611620545387268, 0.5437813401222229, 0.7871934175491333, 0.007358688861131668, 0.5028117895126343, 0.4907846748828888, 0.37655746936798096, 0.25387972593307495, 0.26416951417922974, 0.24430836737155914, 0.5675936341285706, 0.4901319146156311, 0.7996804714202881, 0.7203385233879089, 0.7544000744819641, 0.377120703458786, 0.8940339684486389, 0.6455138921737671, 0.8390150666236877, 0.7330541610717773, 0.6957420110702515, 0.833514392375946, 0.6529996991157532, 0.658237874507904, 0.7534120678901672, 0.8279337882995605, 0.7706102132797241, 0.8231885433197021, 0.16554765403270721, 0.8775724768638611, 0.9194568991661072] ['Blue', 'Red', 'Green', 'Yellow', 'Orange', 'Purple', 'Black', 'White', 'Pink', 'Brown', 'Gray', 'Violet', 'Magenta', 'Teal', 'Indigo', 'Tan', 'Turquoise', 'Silver', 'Lavender', 'Gold', 'Beige', 'Rose', 'Tulip', 'Daisy', 'Lily', 'Sunflower', 'Daffodil', 'Carnation', 'Petunia', 'Orchid', 'Dandelion', 'Pansy', 'Violet', 'Iris', 'Lilac', 'Peony', 'Gardenia', 'Marigold', 'Mum', 'Geranium', 'Hydrangea']\n",
      "    Category   Response  Typicality_Scores\n",
      "178  A color       Blue               0.97\n",
      "179  A color        Red               0.96\n",
      "180  A color      Green               0.92\n",
      "181  A color     Yellow               0.85\n",
      "182  A color     Orange               0.79\n",
      "183  A color     Purple               0.77\n",
      "184  A color      Black               0.72\n",
      "185  A color      White               0.58\n",
      "186  A color       Pink               0.55\n",
      "187  A color      Brown               0.38\n",
      "188  A color       Gray               0.21\n",
      "189  A color     Violet               0.19\n",
      "190  A color    Magenta               0.13\n",
      "191  A color       Teal               0.11\n",
      "192  A color     Indigo               0.10\n",
      "193  A color        Tan               0.09\n",
      "194  A color  Turquoise               0.09\n",
      "195  A color     Silver               0.08\n",
      "196  A color   Lavender               0.07\n",
      "197  A color       Gold               0.06\n",
      "198  A color      Beige               0.06\n",
      "    Category   Response  Typicality_Scores\n",
      "178  A color       Blue               0.97\n",
      "179  A color        Red               0.96\n",
      "180  A color      Green               0.92\n",
      "181  A color     Yellow               0.85\n",
      "182  A color     Orange               0.79\n",
      "183  A color     Purple               0.77\n",
      "184  A color      Black               0.72\n",
      "185  A color      White               0.58\n",
      "186  A color       Pink               0.55\n",
      "187  A color      Brown               0.38\n",
      "188  A color       Gray               0.21\n",
      "189  A color     Violet               0.19\n",
      "190  A color    Magenta               0.13\n",
      "191  A color       Teal               0.11\n",
      "192  A color     Indigo               0.10\n",
      "193  A color        Tan               0.09\n",
      "194  A color  Turquoise               0.09\n",
      "195  A color     Silver               0.08\n",
      "196  A color   Lavender               0.07\n",
      "197  A color       Gold               0.06\n",
      "198  A color      Beige               0.06 [0.97 0.96 0.92 0.85 0.79 0.77 0.72 0.58 0.55 0.38 0.21 0.19 0.13 0.11\n",
      " 0.1  0.09 0.09 0.08 0.07 0.06 0.06 0.19]\n",
      "[0.65810323 0.62984163 0.48608556 0.66818321 0.59267503 0.53171468\n",
      " 0.64074725 0.58919352 0.56339628 0.53137821 0.56116205 0.54378134\n",
      " 0.78719342 0.00735869 0.50281179 0.49078467 0.37655747 0.25387973\n",
      " 0.26416951 0.24430837 0.56759363 0.6529997 ]\n",
      "     Category   Response  Typicality_Scores\n",
      "450  A flower       Rose               0.89\n",
      "451  A flower      Tulip               0.53\n",
      "452  A flower      Daisy               0.50\n",
      "453  A flower       Lily               0.30\n",
      "454  A flower  Sunflower               0.26\n",
      "455  A flower   Daffodil               0.22\n",
      "456  A flower  Carnation               0.22\n",
      "457  A flower    Petunia               0.15\n",
      "458  A flower     Orchid               0.14\n",
      "459  A flower  Dandelion               0.12\n",
      "460  A flower      Pansy               0.11\n",
      "461  A flower     Violet               0.11\n",
      "462  A flower       Iris               0.08\n",
      "463  A flower      Lilac               0.08\n",
      "464  A flower      Peony               0.08\n",
      "465  A flower   Gardenia               0.07\n",
      "466  A flower   Marigold               0.07\n",
      "467  A flower        Mum               0.06\n",
      "468  A flower   Geranium               0.05\n",
      "469  A flower  Hydrangea               0.05\n",
      "     Category   Response  Typicality_Scores\n",
      "450  A flower       Rose               0.89\n",
      "451  A flower      Tulip               0.53\n",
      "452  A flower      Daisy               0.50\n",
      "453  A flower       Lily               0.30\n",
      "454  A flower  Sunflower               0.26\n",
      "455  A flower   Daffodil               0.22\n",
      "456  A flower  Carnation               0.22\n",
      "457  A flower    Petunia               0.15\n",
      "458  A flower     Orchid               0.14\n",
      "459  A flower  Dandelion               0.12\n",
      "460  A flower      Pansy               0.11\n",
      "461  A flower     Violet               0.11\n",
      "462  A flower       Iris               0.08\n",
      "463  A flower      Lilac               0.08\n",
      "464  A flower      Peony               0.08\n",
      "465  A flower   Gardenia               0.07\n",
      "466  A flower   Marigold               0.07\n",
      "467  A flower        Mum               0.06\n",
      "468  A flower   Geranium               0.05\n",
      "469  A flower  Hydrangea               0.05 [0.11 0.89 0.53 0.5  0.3  0.26 0.22 0.22 0.15 0.14 0.12 0.11 0.11 0.08\n",
      " 0.08 0.08 0.07 0.07 0.06 0.05 0.05]\n",
      "[0.54378134 0.49013191 0.79968047 0.72033852 0.75440007 0.3771207\n",
      " 0.89403397 0.64551389 0.83901507 0.73305416 0.69574201 0.83351439\n",
      " 0.6529997  0.65823787 0.75341207 0.82793379 0.77061021 0.82318854\n",
      " 0.16554765 0.87757248 0.9194569 ]\n",
      "Color Category Spearman Correlation: 0.58\n",
      "Flower Category Spearman Correlation: -0.28\n"
     ]
    }
   ],
   "source": [
    "castro_et_al_typicality = pd.read_csv(\"files/castro_et_al_typicality.csv\")\n",
    "## TODO\n",
    "# pass\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "color_data = castro_et_al_typicality[castro_et_al_typicality['Category'] == 'A color']\n",
    "flower_data = castro_et_al_typicality[castro_et_al_typicality['Category'] == 'A flower']\n",
    "\n",
    "color_data = color_data.dropna(subset=['Typicality_Scores'])\n",
    "flower_data = flower_data.dropna(subset=['Typicality_Scores'])\n",
    "\n",
    "similarities, valid_responses = [], []\n",
    "\n",
    "def compute_similarity(response_list, category_word):\n",
    "    category_vector = dict_word_vectors[category_word]\n",
    "    \n",
    "    for response in response_list:\n",
    "        response_vector = dict_word_vectors[response.lower()]\n",
    "        \n",
    "        if response_vector is not None:\n",
    "            similarity = compute_cosine_similarity(response_vector, category_vector)\n",
    "            similarities.append(similarity)\n",
    "            valid_responses.append(response)\n",
    "\n",
    "    return similarities, valid_responses\n",
    "\n",
    "color_similarities, color_responses = compute_similarity(color_data['Response'].tolist(), 'color')\n",
    "flower_similarities, flower_responses = compute_similarity(flower_data['Response'].tolist(), 'flower')\n",
    "\n",
    "def compare_with_human_data(human_data, model_similarities, valid_responses):\n",
    "    human_data = human_data[human_data['Response'].isin(valid_responses)]\n",
    "    response_to_score = dict(zip(human_data['Response'], human_data['Typicality_Scores']))\n",
    "    human_scores = [response_to_score.get(response, np.nan) for response in valid_responses]\n",
    "    valid_indices = ~np.isnan(human_scores)\n",
    "    human_scores = np.array(human_scores)[valid_indices]\n",
    "    model_similarities = np.array(model_similarities)[valid_indices]\n",
    "\n",
    "    return spearmanr(human_scores, model_similarities)\n",
    "\n",
    "color_correlation = compare_with_human_data(color_data, color_similarities, color_responses)\n",
    "flower_correlation = compare_with_human_data(flower_data, flower_similarities, flower_responses)\n",
    "\n",
    "print(f\"Color Category Spearman Correlation: {color_correlation.correlation:.2f}\")\n",
    "print(f\"Flower Category Spearman Correlation: {flower_correlation.correlation:.2f}\")\n",
    "\n",
    "# Color Category Spearman Correlation: -0.43\n",
    "# Flower Category Spearman Correlation: 0.47\n",
    "\n",
    "# Color Category Spearman Correlation: 0.58\n",
    "# Flower Category Spearman Correlation: -0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ae8b0",
   "metadata": {},
   "source": [
    "## Questions on LSA\n",
    "\n",
    "1. LSA uses 300 dimensions for vector representation. Imagine you repeat this assignment but only use first 50 dimensions of vector representations. How do you think this would affect the results on the different tasks?  \n",
    "2. Do you think humans have a vector representation model for words? No, why not? Yes, then how do humans learn these vector representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb3ef65-bba7-4c84-8f27-4899f39f91ac",
   "metadata": {},
   "source": [
    "You're done now! Submit the assignment to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacea18b-786b-4f57-ae62-226aed9e174a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Castro, N., Curley, T. M., & Hertzog, C. (2020, April 21). Category norms with a cross-sectional sample of adults in the United States: Consideration of cohort, age, and historical effects on semantic categories. https://doi.org/10.31234/osf.io/4gzn6\n",
    "\n",
    "[Günther, F., Dudschig, C., & Kaup, B. (2015). LSAfun – An R package for computations based on Latent Semantic Analysis. Behavior Research Methods, 47, 930-944.](https://sites.google.com/site/fritzgntr/software-resources/semantic_spaces)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "name": "assignment-3-vector-representations-and-lsa.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
