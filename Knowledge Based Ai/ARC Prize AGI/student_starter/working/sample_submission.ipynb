{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c05628-c946-421d-8dff-95c5410d5e14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sample ARC Submission\n",
    "\n",
    "This is a sample notebook that can help get you started with creating an ARC Prize submission. It covers the basics of loading libraries, loading data, implementing an approach, and submitting.\n",
    "\n",
    "You should be able to submit this notebook to the evaluation portal and have it run successfully (although you'll get a score of 0, so you'll need to do some work if you want to do better!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717270ed-182b-4598-8f12-612cd62de60e",
   "metadata": {},
   "source": [
    "# Load needed libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828dd6cc-7da7-4bec-a211-ff9afb2e688a",
   "metadata": {},
   "source": [
    "Basic libraries like numpy, torch, matplotlib, and tqdm are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca3d361d-d3e0-45d2-b416-803c0ad1822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae22af4-5a95-40d1-ac1f-f611b77578c7",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4bf5a-3237-46f6-ab05-2319a0497a8b",
   "metadata": {},
   "source": [
    "Here we are loading the training challenges and solutions (this is the public training set), the evaluation challenges and solutions (this is the public evaluation set), and the test challenges (currently a placeholder file that is a copy of the public evaluation challanges).\n",
    "\n",
    "For your initial testing and exploration, I'd recommend not using the public evaluation set, just work off the public training set and then test against the test challenges (which is actually the public evaluation set). However, when competing in the competition, then you can should probably use the evaluation tasks for training too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "207605ea-58a7-46e0-9fd2-3ee2000eece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public training set\n",
    "train_challenges_path = '../input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solutions_path = '../input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "\n",
    "with open(train_challenges_path) as fp:\n",
    "    train_challenges = json.load(fp)\n",
    "with open(train_solutions_path) as fp:\n",
    "    train_solutions = json.load(fp)\n",
    "\n",
    "# Public evaluation set\n",
    "evaluation_challenges_path = '../input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "evaluation_solutions_path = '../input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "\n",
    "with open(evaluation_challenges_path) as fp:\n",
    "    evaluation_challenges = json.load(fp)\n",
    "with open(evaluation_solutions_path) as fp:\n",
    "    evaluation_solutions = json.load(fp)\n",
    "\n",
    "# This will be the hidden test challenges (currently has a placeholder to the evaluation set)\n",
    "test_challenges_path = '../input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "\n",
    "with open(test_challenges_path) as fp:\n",
    "    test_challenges = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292806c-8c0c-4efd-9c30-b3660254e4c2",
   "metadata": {},
   "source": [
    "Here is an example of what a test task looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be570a8b-7105-4935-b538-5e58794406f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': [{'input': [[3, 2], [7, 8]]}],\n",
       " 'train': [{'input': [[8, 6], [6, 4]],\n",
       "   'output': [[8, 6, 8, 6, 8, 6],\n",
       "    [6, 4, 6, 4, 6, 4],\n",
       "    [6, 8, 6, 8, 6, 8],\n",
       "    [4, 6, 4, 6, 4, 6],\n",
       "    [8, 6, 8, 6, 8, 6],\n",
       "    [6, 4, 6, 4, 6, 4]]},\n",
       "  {'input': [[7, 9], [4, 3]],\n",
       "   'output': [[7, 9, 7, 9, 7, 9],\n",
       "    [4, 3, 4, 3, 4, 3],\n",
       "    [9, 7, 9, 7, 9, 7],\n",
       "    [3, 4, 3, 4, 3, 4],\n",
       "    [7, 9, 7, 9, 7, 9],\n",
       "    [4, 3, 4, 3, 4, 3]]}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_task = list(test_challenges.keys())[0]\n",
    "test_challenges[sample_task]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f34083-882b-4fb9-b5fd-92dd75772dc3",
   "metadata": {},
   "source": [
    "# Generating a submission\n",
    "\n",
    "To generate a submission you need to output a file called `submission.json` that has the following format:\n",
    "\n",
    "```\n",
    "{\"00576224\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " \"009d5c81\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " \"12997ef3\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]},\n",
    "              {\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " ...\n",
    "}\n",
    "```\n",
    "\n",
    "In this case, the task ids come from `test_challenges`. There may be multiple (i.e., >1) test items per task. Therefore, the dictionary has a list of dicts for each task. These submission dictionaries should appear in the same order as the test items from `test_challenges`. Additionally, you can provide two attempts for each test item. In fact, you **MUST** provide two attempts. If you only want to generate a single attempt, then just submit the same answer for both attempts (or submit an empty submission like the ones shown in the example snippit just above.\n",
    "\n",
    "Here is how we might create a blank submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d53c553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(input_data):\n",
    "    # Extract the input pairs\n",
    "    row1, row2 = input_data\n",
    "\n",
    "    # Create the output matrix\n",
    "    output = []\n",
    "    \n",
    "    # Row pattern 1: Repeat row1 three times horizontally\n",
    "    output.append(row1 * 3)\n",
    "    output.append(row2 * 3)\n",
    "    \n",
    "    # Row pattern 2: Repeat row2 and row1 vertically three times\n",
    "    output.append([row1[1], row1[0]] * 3)\n",
    "    output.append([row2[1], row2[0]] * 3)\n",
    "    \n",
    "    # Repeat the initial rows to form the complete 6x6 grid\n",
    "    output.append(row1 * 3)\n",
    "    output.append(row2 * 3)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e25a0fb-e206-4505-b597-8863c41c7e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 138505.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty submission dict for output\n",
    "submission = {}\n",
    "\n",
    "# iterate over the test items and build up submission answers\n",
    "count = 0\n",
    "for key, task in tqdm(test_challenges.items()):\n",
    "\n",
    "    # Here are the task's training inputs and outputs\n",
    "    train_inputs = [item['input'] for item in task['train']]\n",
    "    train_outputs = [item['output'] for item in task['train']]\n",
    "\n",
    "    # Here we generate outputs for each test item.\n",
    "    submission[key] = []\n",
    "\n",
    "    # this is just a placeholder, but would be where you might generate your predictions.\n",
    "    # blank_prediction = [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]\n",
    "    blank_prediction = [[0, 0], [0, 0]]\n",
    "    for test_item in task['test']:\n",
    "        input_grid = test_item['input']\n",
    "        if(len(input_grid) == 2):\n",
    "            blank_prediction = generate_output(input_grid)\n",
    "            \n",
    "        submission[key] = [{'attempt_1': blank_prediction, 'attempt_2': blank_prediction} for item in task['test']]\n",
    "\n",
    "# Here we write the submissions to file, so that they will get evaluated\n",
    "with open('submission.json', 'w') as fp:\n",
    "    json.dump(submission, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3d827-8f78-47d4-8bc7-3c07cfc996c2",
   "metadata": {},
   "source": [
    "Here is what our submission for the test task above looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "07d83daa-496f-4cad-8550-876d115c4a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'attempt_1': [[3, 2, 3, 2, 3, 2],\n",
       "   [7, 8, 7, 8, 7, 8],\n",
       "   [2, 3, 2, 3, 2, 3],\n",
       "   [8, 7, 8, 7, 8, 7],\n",
       "   [3, 2, 3, 2, 3, 2],\n",
       "   [7, 8, 7, 8, 7, 8]],\n",
       "  'attempt_2': [[3, 2, 3, 2, 3, 2],\n",
       "   [7, 8, 7, 8, 7, 8],\n",
       "   [2, 3, 2, 3, 2, 3],\n",
       "   [8, 7, 8, 7, 8, 7],\n",
       "   [3, 2, 3, 2, 3, 2],\n",
       "   [7, 8, 7, 8, 7, 8]]}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[sample_task]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f7599",
   "metadata": {},
   "source": [
    "# Scoring Your Submission\n",
    "\n",
    "If you do not want to wait for gradescope to score your solution, we have provided the following code to score your submission. Note that the maximum possibe score is 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c39df0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def score_submission():\n",
    "    with open('../input/arc-prize-2024/arc-agi_evaluation_solutions.json', 'r') as sol_file:\n",
    "        solutions = json.load(sol_file)\n",
    "    \n",
    "    with open('submission.json', 'r') as sub_file:\n",
    "        submission = json.load(sub_file)\n",
    "    \n",
    "    overall_score = 0\n",
    "\n",
    "    for task in solutions:\n",
    "        score = 0\n",
    "        for i, answer in enumerate(solutions[task]):\n",
    "            # try:\n",
    "                \n",
    "            # except:\n",
    "            #     # Handle any other exceptions\n",
    "            #     pass\n",
    "            # else:\n",
    "            #     # Code to execute if no exceptions are raised\n",
    "            #     pass\n",
    "            # finally:\n",
    "            #     # Code to execute whether an exception was raised or not\n",
    "            #     pass\n",
    "            attempt1_correct = submission[task][i]['attempt_1'] == answer\n",
    "            attempt2_correct = submission[task][i]['attempt_2'] == answer\n",
    "            score += int(attempt1_correct or attempt2_correct)\n",
    "        score /= len(solutions[task])\n",
    "\n",
    "        overall_score += score\n",
    "    \n",
    "\n",
    "    print(overall_score)\n",
    "\n",
    "score_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6818bb2",
   "metadata": {},
   "source": [
    "You can run the above code by uncommenting the following code block. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
